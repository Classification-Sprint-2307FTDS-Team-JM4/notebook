{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50100466",
   "metadata": {},
   "source": [
    "# ADVANCED CLASSIFICATION PREDICT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfae2a7",
   "metadata": {},
   "source": [
    "# Predict Overview: EA Twitter Sentiment Classification\n",
    "Companies would like to determine how people perceive climate change and whether or not they believe it is a real threat. \n",
    "\n",
    "Our mission is to deliver a precise and durable solution to this objective, granting companies the ability to tap into a wide range of consumer sentiments across various demographics and geographic regions. This, in turn, enhances their understanding and empowers them to shape future marketing strategies based on valuable insights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f910cebe",
   "metadata": {},
   "source": [
    "<a id=\"cont\"></a>\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "<a href=#one>1. Importing Packages</a>\n",
    "\n",
    "<a href=#two>2. Loading Data</a>\n",
    "\n",
    "<a href=#three>3. Exploratory Data Analysis (EDA)</a>\n",
    "\n",
    "<a href=#four>4. Data Engineering</a>\n",
    "\n",
    "<a href=#five>5. Modeling</a>\n",
    "\n",
    "<a href=#six>6. Model Performance</a>\n",
    "\n",
    "<a href=#seven>7. Model Explanations</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846337c4",
   "metadata": {},
   "source": [
    "<a id=\"one\"></a>\n",
    "\n",
    "## 1. Importing Packages\n",
    "\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "\n",
    "| ⚡ Description: Importing Packages ⚡                                                                                                    |\n",
    "| :--------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| In this section you are required to import, and briefly discuss, the libraries that will be used throughout your analysis and modelling. |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5dd3acaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\percy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\percy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\percy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\percy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\percy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\percy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.style as style\n",
    "%matplotlib inline\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "nltk.download('stopwords')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer \n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1a68b3",
   "metadata": {},
   "source": [
    "<a id=\"two\"></a>\n",
    "\n",
    "## 2. Loading the Data\n",
    "\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "\n",
    "| ⚡ Description: Loading the data ⚡                                                          |\n",
    "| :------------------------------------------------------------------------------------------- |\n",
    "| In this section you are required to load the data from the `df_train` file into a DataFrame. |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a22bd1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "      <th>tweetid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Europe will now be looking to China to make su...</td>\n",
       "      <td>169760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Combine this with the polling of staffers re c...</td>\n",
       "      <td>35326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The scary, unimpeachable evidence that climate...</td>\n",
       "      <td>224985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@Karoli @morgfair @OsborneInk @dailykos \\nPuti...</td>\n",
       "      <td>476263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RT @FakeWillMoore: 'Female orgasms cause globa...</td>\n",
       "      <td>872928</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             message  tweetid\n",
       "0  Europe will now be looking to China to make su...   169760\n",
       "1  Combine this with the polling of staffers re c...    35326\n",
       "2  The scary, unimpeachable evidence that climate...   224985\n",
       "3  @Karoli @morgfair @OsborneInk @dailykos \\nPuti...   476263\n",
       "4  RT @FakeWillMoore: 'Female orgasms cause globa...   872928"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.read_csv (r\"C:\\Users\\percy\\Downloads\\Advanced_Classification_Predict-student_data-2780\\test_with_no_labels.csv\")\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25358699",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>message</th>\n",
       "      <th>tweetid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>PolySciMajor EPA chief doesn't think carbon di...</td>\n",
       "      <td>625221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>It's not like we lack evidence of anthropogeni...</td>\n",
       "      <td>126103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>RT @RawStory: Researchers say we have three ye...</td>\n",
       "      <td>698562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>#TodayinMaker# WIRED : 2016 was a pivotal year...</td>\n",
       "      <td>573736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>RT @SoyNovioDeTodas: It's 2016, and a racist, ...</td>\n",
       "      <td>466954</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                            message  tweetid\n",
       "0          1  PolySciMajor EPA chief doesn't think carbon di...   625221\n",
       "1          1  It's not like we lack evidence of anthropogeni...   126103\n",
       "2          2  RT @RawStory: Researchers say we have three ye...   698562\n",
       "3          1  #TodayinMaker# WIRED : 2016 was a pivotal year...   573736\n",
       "4          1  RT @SoyNovioDeTodas: It's 2016, and a racist, ...   466954"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv(r\"C:\\Users\\percy\\Downloads\\Advanced_Classification_Predict-student_data-2780\\train.csv\")\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfa4630",
   "metadata": {},
   "source": [
    "<a id=\"three\"></a>\n",
    "\n",
    "## 3. Exploratory Data Analysis (EDA)\n",
    "\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "\n",
    "| ⚡ Description: Exploratory data analysis ⚡                                                             |\n",
    "| :------------------------------------------------------------------------------------------------------- |\n",
    "| In this section, you are required to perform an in-depth analysis of all the variables in the DataFrame. |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703c0485",
   "metadata": {},
   "source": [
    "The EDA  includes:\n",
    "- Summary analysis\n",
    "- Null values\n",
    "- Classes of tweets - Sentiments\n",
    "- Number of words of tweets\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3c219e",
   "metadata": {},
   "source": [
    "## Summary analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c429a084",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11832a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9dd6a1c",
   "metadata": {},
   "source": [
    "## Null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5cedc5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb82547",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cab8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9beb2d88",
   "metadata": {},
   "source": [
    "## Sentiments meaning\n",
    "- Anti climate Change = -1\n",
    "- Neutral = 0\n",
    "- Pro Climate change = 1\n",
    "- News = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410609e7",
   "metadata": {},
   "source": [
    "## 3.1 Comparing the number of tweets of categories\n",
    "- Distribution\n",
    "- Pie chart\n",
    "- Word cloud\n",
    "- Top 20 words used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfba0cd6",
   "metadata": {},
   "source": [
    "### 3.1.1 Bar chart showing the counts of tweets per sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691b0153",
   "metadata": {},
   "outputs": [],
   "source": [
    "senti_counts = df_train[\"sentiment\"].value_counts()\n",
    "news = senti_counts[2] \n",
    "pro = senti_counts[1]   \n",
    "neutral = senti_counts[0]\n",
    "anti = senti_counts[-1]  \n",
    "\n",
    "plt.figure( figsize=(9,4))\n",
    "plt.barh(['News ','Pro','Neutral','Anti'], [news,pro,neutral,anti]) # horizontal bar graph to compare classes of tweets.\n",
    "plt.colours = ['red', 'green', 'blue', 'orange']\n",
    "plt.xlabel('Count of Tweets') #X-label of the data\n",
    "plt.ylabel('Sentiment Classification') #Y_label of the data \n",
    "plt.title('Distribution of Classes In The Dataset') #Give the data a title 'Dataset lables distribution'\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3bd3521",
   "metadata": {},
   "source": [
    "### 3.1.2 Pie chart showing the percentages of tweets per sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b443fc9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_counts = df_train[\"sentiment\"].value_counts()\n",
    "plt.figure( figsize=(9,4))\n",
    "plt.pie(class_counts, labels=['News ','Pro','Neutral','Anti'], explode=[0.05,0.05,0.05,0.05], autopct='%1.1f%%')\n",
    "plt.title(\"Pie Chart of Percentage classification of Tweets \")\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39fa5a2a",
   "metadata": {},
   "source": [
    "Insights:\n",
    "- News has the highest percentage of tweets followed by Pro climate change, Neutral and anti climate change respectively"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9019cf92",
   "metadata": {},
   "source": [
    "### 3.1.3 Word cloud showing top words used in every class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b183102c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud,ImageColorGenerator\n",
    "text = \" \".join(i for i in df_train[\"message\"])\n",
    "text = str(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be44e3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud()\n",
    "tweet_cloud = wordcloud.generate(text)\n",
    "plt.figure( figsize=(9,4))\n",
    "plt.imshow(tweet_cloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ec1165",
   "metadata": {},
   "source": [
    " ### 3.1.4 Top 20 words in the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87060826",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud,ImageColorGenerator\n",
    "text = \" \".join(i for i in df_train[\"message\"])\n",
    "text = str(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59a701f",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_dict = {}\n",
    "words = text.split(\" \")\n",
    "for word in words:\n",
    "    if word != \" \" and word !=\"\":\n",
    "        if word not in freq_dict:\n",
    "            freq_dict[word] = 1\n",
    "        else:\n",
    "            freq_dict[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830608f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_freq_dict = sorted(freq_dict.items(), key=lambda x:x[1], reverse=True)[:21]\n",
    "top_20_words = dict(sorted_freq_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3ff8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_labels = list(top_20_words.keys())\n",
    "values = list(top_20_words.values())\n",
    "\n",
    "# Create the figure and axes\n",
    "fig, ax = plt.subplots(figsize=(9,4))\n",
    "\n",
    "# Plot the data\n",
    "ax.bar(x_labels, values)\n",
    "\n",
    "# Customize the plot\n",
    "ax.set_xlabel('word')\n",
    "ax.set_ylabel('Word Count')\n",
    "ax.set_title('Top 20 Most Used Words')\n",
    "\n",
    "# Rotate the x-labels if needed\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d71a421",
   "metadata": {},
   "source": [
    "The top of words used\n",
    "- 1. Climate\n",
    "- 2. Change\n",
    "- 3. rt\n",
    "- 4. Trump\n",
    "- 5. Global\n",
    "- 6. US\n",
    "- 7. via\n",
    "- 8. Says\n",
    "- 9. EPA\n",
    "- 10. New\n",
    "- 11. Scientists\n",
    "- 12. Trumps\n",
    "- 13. Donald\n",
    "- 14. Fight\n",
    "- 15. China\n",
    "- 16. Could\n",
    "- 17. News\n",
    "- 18. world\n",
    "- 19. Scott\n",
    "- 20. Paris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abcf894a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud,ImageColorGenerator\n",
    "text = \" \".join(i for i in df_anti[\"message\"])\n",
    "text = str(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9506cb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_dict = {}\n",
    "words = text.split(\" \")\n",
    "for word in words:\n",
    "    if word != \" \" and word !=\"\":\n",
    "        if word not in freq_dict:\n",
    "            freq_dict[word] = 1\n",
    "        else:\n",
    "            freq_dict[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796ef04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_freq_dict = sorted(freq_dict.items(), key=lambda x:x[1], reverse=True)[:21]\n",
    "top_20_words = dict(sorted_freq_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a018b252",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_labels = list(top_20_words.keys())\n",
    "values = list(top_20_words.values())\n",
    "\n",
    "# Create the figure and axes\n",
    "fig, ax = plt.subplots(figsize=(9,4))\n",
    "\n",
    "# Plot the data\n",
    "ax.bar(x_labels, values)\n",
    "\n",
    "# Customize the plot\n",
    "ax.set_xlabel('word')\n",
    "ax.set_ylabel('Word Count')\n",
    "ax.set_title('Top 20 Most Used Words')\n",
    "\n",
    "# Rotate the x-labels if needed\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a103fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud,ImageColorGenerator\n",
    "text = \" \".join(i for i in df_pro[\"message\"])\n",
    "text = str(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570d4941",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_dict = {}\n",
    "words = text.split(\" \")\n",
    "for word in words:\n",
    "    if word != \" \" and word !=\"\":\n",
    "        if word not in freq_dict:\n",
    "            freq_dict[word] = 1\n",
    "        else:\n",
    "            freq_dict[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee929d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_freq_dict = sorted(freq_dict.items(), key=lambda x:x[1], reverse=True)[:21]\n",
    "top_20_words = dict(sorted_freq_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65df0caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_labels = list(top_20_words.keys())\n",
    "values = list(top_20_words.values())\n",
    "\n",
    "# Create the figure and axes\n",
    "fig, ax = plt.subplots(figsize=(9,4))\n",
    "\n",
    "# Plot the data\n",
    "ax.bar(x_labels, values)\n",
    "\n",
    "# Customize the plot\n",
    "ax.set_xlabel('word')\n",
    "ax.set_ylabel('Word Count')\n",
    "ax.set_title('Top 20 Most Used Words')\n",
    "\n",
    "# Rotate the x-labels if needed\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12557949",
   "metadata": {},
   "source": [
    "## 3.2 Comparing the *length of the tweet* per categories\n",
    "- Box plots\n",
    "- Distribution bar graphs\n",
    "- Summary of Stats\n",
    "- Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76df533",
   "metadata": {},
   "source": [
    "### 3.2.1 Box plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df97e95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "length = [len(tweet) for tweet in df_train['message']]\n",
    "df_train['length'] = length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b38fb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating class subsets for the datase\n",
    "\n",
    "df_anti = df_train.copy()[df_train['sentiment'] == -1]\n",
    "df_neutral = df_train.copy()[df_train['sentiment'] == 0]\n",
    "df_pro = df_train.copy()[df_train['sentiment'] == 1]\n",
    "df_news = df_train.copy()[df_train['sentiment'] == 2]\n",
    "\n",
    "#storing the size data in separate variables\n",
    "\n",
    "pro_len = df_pro['length']\n",
    "neutral_len = df_neutral['length']\n",
    "anti_len = df_anti['length']\n",
    "news_len = df_news['length']\n",
    "data_len = df_train['length']\n",
    "\n",
    "#creating a list of all the length datasets\n",
    "\n",
    "len_data = [pro_len, anti_len, neutral_len, news_len, data_len]\n",
    "\n",
    "# Create a figure and axis\n",
    "fig, ax = plt.subplots(figsize=(9,4))\n",
    "\n",
    "# Create the box plots\n",
    "ax.boxplot(len_data, vert=False)\n",
    "\n",
    "# Set the labels for each box plot\n",
    "labels = ['pro', 'anti', 'neutral', 'news', 'main data']\n",
    "ax.set_yticklabels(labels)\n",
    "\n",
    "# Set the title and axis labels\n",
    "plt.title('Box and Whiskers Diagram For Tweet Lengths Per Category')\n",
    "plt.xlabel('Length In Characters')\n",
    "plt.ylabel('Class of Tweet')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998f1515",
   "metadata": {},
   "source": [
    "### 3.2.2 Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6718d0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('mean of pro-climate is:  ', round(df_pro['length'].mean(),2))\n",
    "round(df_pro['length'].describe(),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea54c845",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('mean of anti-climate is:  ', round(df_anti['length'].mean(),2))\n",
    "round(df_anti['length'].describe(),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d43b3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('mean of neutral-climate is:  ', round(df_neutral['length'].mean(),2))\n",
    "round(df_neutral['length'].describe(),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdeeca4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('mean of news is:  ', round(df_news['length'].mean(),2))\n",
    "round(df_news['length'].describe(),2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfe6082",
   "metadata": {},
   "source": [
    "### 3.2.3 Bar graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a01b507",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure( figsize=(9,4))\n",
    "plt.hist(df_train['length'])\n",
    "plt.title(\"Distribution of Tweet Lengths\")\n",
    "plt.xlabel(\"Length of Tweet In Charaters\") #X-label of the data\n",
    "plt.ylabel(\"Number of Tweets\")      #Y_label of the data\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e8be9c",
   "metadata": {},
   "source": [
    "### 3.2.4 Correlation between length of tweets and sentiment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12733366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the correlation matrix\n",
    "correlation_matrix = df_train.corr()\n",
    "\n",
    "# Calculate the correlations with the target variable\n",
    "target_correlations = correlation_matrix['sentiment']\n",
    "\n",
    "# Sort the features by their correlations with the target variable\n",
    "sorted_features = target_correlations.abs().sort_values(ascending=False).index\n",
    "\n",
    "# Reorder the correlation matrix\n",
    "sorted_corr_matrix = correlation_matrix.loc[sorted_features, sorted_features]\n",
    "\n",
    "# Create a heatmap for the sorted correlations without annotations\n",
    "plt.figure(figsize=(7, 5))\n",
    "sns.heatmap(sorted_corr_matrix, cmap='coolwarm')\n",
    "plt.title('Correlation with sentiments')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a30857",
   "metadata": {},
   "source": [
    "## Retweets "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a77eb3b",
   "metadata": {},
   "source": [
    "<a id=\"four\"></a>\n",
    "\n",
    "## 4. Data Engineering\n",
    "\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "\n",
    "| ⚡ Description: Data engineering ⚡                                                                                        |\n",
    "| :------------------------------------------------------------------------------------------------------------------------- |\n",
    "| In this section you are required to: clean the dataset, and possibly create new features - as identified in the EDA phase. |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4638a5c",
   "metadata": {},
   "source": [
    "Data engineering involves data cleaning and feature engineering\n",
    "\n",
    "## 4.1 Data cleaning\n",
    "In order to process the tweet messages more effectively the tweets are cleaned using the clean function defined in the code cell below. The clean function does the following.\n",
    "\n",
    "- Convert all tweet text to lowercase.\n",
    "- Drop colums that are not important\n",
    "- Remove @\n",
    "- Remove punctuation.\n",
    "- Remove numbers\n",
    "- Remove stopwords\n",
    "- Remove line-break code syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60c65ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwordlist = ['a', 'about', 'above', 'after', 'again', 'ain', 'all', 'am', 'an',\n",
    "             'and','any','are', 'as', 'at', 'be', 'because', 'been', 'before',\n",
    "             'being', 'below', 'between','both', 'by', 'can', 'd', 'did', 'do',\n",
    "             'does', 'doing', 'down', 'during', 'each','few', 'for', 'from',\n",
    "             'further', 'had', 'has', 'have', 'having', 'he', 'her', 'here',\n",
    "             'hers', 'herself', 'him', 'himself', 'his', 'how', 'i', 'if', 'in',\n",
    "             'into','is', 'it', 'its', 'itself', 'just', 'll', 'm', 'ma',\n",
    "             'me', 'more', 'most','my', 'myself', 'now', 'o', 'of', 'on', 'once',\n",
    "             'only', 'or', 'other', 'our', 'ours','ourselves', 'out', 'own', 're','s', 'same', 'she', \"shes\", 'should', \"shouldve\",'so', 'some', 'such',\n",
    "             't', 'than', 'that', \"thatll\", 'the', 'their', 'theirs', 'them',\n",
    "             'themselves', 'then', 'there', 'these', 'they', 'this', 'those',\n",
    "             'through', 'to', 'too','under', 'until', 'up', 've', 'very', 'was',\n",
    "             'we', 'were', 'what', 'when', 'where','which','while', 'who', 'whom',\n",
    "             'why', 'will', 'with', 'won', 'y', 'you', \"youd\",\"youll\", \"youre\",\n",
    "             \"youve\", 'your', 'yours', 'yourself', 'yourselves']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3581961c",
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS = set(stopwordlist)\n",
    "def cleaning_stopwords(text):\n",
    "    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n",
    "df_train['message'] = df_train['message'].apply(lambda text: cleaning_stopwords(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f5dcad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword=set(stopwords.words('english'))\n",
    "def clean(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    text = [word for word in text.split(' ') if word not in stopword]\n",
    "    text=\" \".join(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e38288f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"message\"] = df_train[\"message\"].apply(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "643fe9bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>message</th>\n",
       "      <th>tokens</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>polyscimajor epa chief doesnt think carbon dio...</td>\n",
       "      <td>[polyscimajor, epa, chief, doesnt, think, carb...</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>like lack evidence anthropogenic global warming</td>\n",
       "      <td>[like, lack, evidence, anthropogenic, global, ...</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>rt rawstory researchers say three years act cl...</td>\n",
       "      <td>[rt, rawstory, researchers, say, three, years,...</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>todayinmaker wired   pivotal year war climate ...</td>\n",
       "      <td>[todayinmaker, wired, pivotal, year, war, clim...</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>rt soynoviodetodas  racist sexist climate chan...</td>\n",
       "      <td>[rt, soynoviodetodas, racist, sexist, climate,...</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                            message  \\\n",
       "0          1  polyscimajor epa chief doesnt think carbon dio...   \n",
       "1          1    like lack evidence anthropogenic global warming   \n",
       "2          2  rt rawstory researchers say three years act cl...   \n",
       "3          1  todayinmaker wired   pivotal year war climate ...   \n",
       "4          1  rt soynoviodetodas  racist sexist climate chan...   \n",
       "\n",
       "                                              tokens  length  \n",
       "0  [polyscimajor, epa, chief, doesnt, think, carb...      95  \n",
       "1  [like, lack, evidence, anthropogenic, global, ...      47  \n",
       "2  [rt, rawstory, researchers, say, three, years,...      70  \n",
       "3  [todayinmaker, wired, pivotal, year, war, clim...      53  \n",
       "4  [rt, soynoviodetodas, racist, sexist, climate,...      90  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c245633d",
   "metadata": {},
   "source": [
    "Tweet id column has no siginificance in the project as it only shows unique twitter number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33f5bf0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.drop('tweetid', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ddb008d",
   "metadata": {},
   "source": [
    "### Cleaning also included:\n",
    "- Tokenization\n",
    "- Stemming\n",
    "- Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c67335",
   "metadata": {},
   "source": [
    "####  Applying Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9b436c",
   "metadata": {},
   "source": [
    "A tokeniser divides text into a sequence of tokens, which roughly correspond to \"words\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9514d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, TreebankWordTokenizer\n",
    "tokeniser = TreebankWordTokenizer()\n",
    "df_train['tokens'] = df_train['message'].apply(tokeniser.tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d3db94",
   "metadata": {},
   "source": [
    "#### Applying stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe63c78",
   "metadata": {},
   "source": [
    "Stemming is the process of transforming to the root word. It uses an algorithm that removes common word-endings from English words, such as “ly,” “es,” “ed,” and “s.”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9db1f59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "st = nltk.PorterStemmer()\n",
    "def stemming_on_text(data):\n",
    "    text = [st.stem(word) for word in data]\n",
    "    return data\n",
    "df_train['message']= df_train['message'].apply(lambda x: stemming_on_text(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3835ab5",
   "metadata": {},
   "source": [
    "#### Applying lemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2281ebf",
   "metadata": {},
   "source": [
    "Lemmatizing is the process of grouping words of similar meaning together. So, your root stem, meaning the word you end up with, is not something you can just look up in a dictionary, but you can look up a lemma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b37d032",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = nltk.WordNetLemmatizer()\n",
    "def lemmatizer_on_text(data):\n",
    "    text = [lm.lemmatize(word) for word in data]\n",
    "    return data\n",
    "df_train['message'] = df_train['message'].apply(lambda x: lemmatizer_on_text(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c7c8ab",
   "metadata": {},
   "source": [
    "## 4.2 Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49fa68d",
   "metadata": {},
   "source": [
    "We added the following features to modify our and for more insights:\n",
    "- Lengths of tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0dfb68",
   "metadata": {},
   "source": [
    "### Length tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d5066c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "length = [len(tweet) for tweet in df_train['message']]\n",
    "df_train['length'] = length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68345fd",
   "metadata": {},
   "source": [
    "<a id=\"five\"></a>\n",
    "\n",
    "## 5. Modelling\n",
    "\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "\n",
    "| ⚡ Description: Modelling ⚡                                                                                                                |\n",
    "| :------------------------------------------------------------------------------------------------------------------------------------------ |\n",
    "| In this section, you are required to create one or more regression models that are able to accurately predict the thee hour load shortfall. |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153a0867",
   "metadata": {},
   "source": [
    "*Pre processing*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4082f0ef",
   "metadata": {},
   "source": [
    "The line X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=50) is splitting the data into training and validation sets.\n",
    "\n",
    "X and y are the feature matrix and target variable, respectively. X contains the features we use to make predictions, and y contains the corresponding target values (in this case, sentiment).\n",
    "\n",
    "train_test_split() is a function provided by scikit-learn (a popular machine learning library) that is used to split a dataset into training and validation subsets.\n",
    "\n",
    "The function takes the following arguments:\n",
    "\n",
    "X: The feature matrix (independent variables). y: The target variable (dependent variable). test_size: This parameter specifies the proportion of the data that should be used for the validation set. In this case, it's set to 0.2, meaning 20% of the data will be used for validation, and the remaining 80% for training. random_state: This is a seed for the random number generator used in the data splitting process. Setting this to a specific value (e.g., 42) ensures that the split is reproducible.\n",
    "\n",
    "train_test_split() returns four sets of data:\n",
    "\n",
    "X_train: This contains the feature data for the training set. X_val: This contains the feature data for the validation set. y_train: This contains the target data for the training set. y_val: This contains the target data for the validation set. By splitting the data into training and validation sets, you can train your machine learning model on a portion of the data (X_train and y_train) and evaluate its performance on another portion that it hasn't seen during training (X_val and y_val). This allows us to estimate how well your model is likely to perform on unseen data (seen in Model Performance section)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "87e48a82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12655,)\n",
      "(3164,)\n",
      "(12655,)\n",
      "(3164,)\n"
     ]
    }
   ],
   "source": [
    "#Splitting features and target variables\n",
    "X = df_train['message'] #X is the features of the cleaned tweets\n",
    "y = df_train['sentiment']    #Y is the target variable which is the train sentiment\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42) #Splitting train set into training and testing data\n",
    "#Print out the shape of the training set and the testing set\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a9aa5c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e3b21090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating the 80% data for training data and 20% for testing data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.2, random_state =42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5180020",
   "metadata": {},
   "source": [
    "## 5.1 Logistic Regression\n",
    "it makes use of a common S-shaped curve known as the logistic function. This curve is commonly known as a sigmoid. It solves the problem for the following reasons:\n",
    "\n",
    "It squeezes the range of output values to exist only between 0 and 1.\n",
    "It has a point of inflection, which can be used to separate the feature space into two distinct areas (one for each class).\n",
    "It has shallow gradients at both its top and bottom, which can be mapped to zeroes or ones respectively with little ambiguity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "313a219f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\percy\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.7424146649810367\n",
      "f1_score 0.7335626779656659\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.78      0.46      0.58       278\n",
      "           0       0.54      0.43      0.48       425\n",
      "           1       0.77      0.86      0.82      1755\n",
      "           2       0.74      0.75      0.74       706\n",
      "\n",
      "    accuracy                           0.74      3164\n",
      "   macro avg       0.71      0.62      0.65      3164\n",
      "weighted avg       0.74      0.74      0.73      3164\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression #Import Logistic Regression from the sklearn\n",
    "model = LogisticRegression(multi_class='ovr' ) #Call logistic regression and assign it to the 'model' variable\n",
    "\n",
    "text_lr= Pipeline([('cf', cf),('clf',model)]) #Create a pipeline of the bag of words and the logistic regression\n",
    "\n",
    "text_lr.fit(X_train, y_train) #Fit the training data to the pipeline\n",
    "\n",
    "y_pred = text_lr.predict(X_test) #Make a prediction of the test set\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test)) #Print the accuracy\n",
    "print('f1_score %s' % f1_score(y_test,y_pred,average='weighted')) #Print the f1-score\n",
    "print(classification_report(y_test, y_pred)) #Print out the classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "212537a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "de96de6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bfece3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import and call the TFidfVectorizer \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer #Import TFidfVectorizer from sklearn\n",
    "tfidf = TfidfVectorizer() #Call the TFidfVectorizer and assign it to the tfidf variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b8462656",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer #Import CountVectorizer from sklearn\n",
    "\n",
    "cf= CountVectorizer() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "067aee7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.7231352718078382\n",
      "f1_score 0.6951149568161584\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.83      0.23      0.37       278\n",
      "           0       0.64      0.32      0.42       425\n",
      "           1       0.71      0.92      0.80      1755\n",
      "           2       0.77      0.67      0.72       706\n",
      "\n",
      "    accuracy                           0.72      3164\n",
      "   macro avg       0.74      0.54      0.58      3164\n",
      "weighted avg       0.73      0.72      0.70      3164\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "model = LogisticRegression(multi_class='ovr') #Call the Logistic Regression model and assign it to the variable 'model'\n",
    "\n",
    "clf = Pipeline([('tfidf', tfidf), ('clf', model)]) #Create a pipeline with the TF-IDF Vectorizer with the logistic model\n",
    "\n",
    "\n",
    "clf.fit(X_train, y_train) #Fit the training data to the pipeline\n",
    "\n",
    "y_pred= clf.predict(X_test) #Make predictions and assign the predictions to y_pred\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test)) #Print the accuracy\n",
    "print('f1_score %s' % f1_score(y_test,y_pred,average='weighted')) #Print the weighted f1 score\n",
    "print(classification_report(y_test, y_pred)) #Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ef0d59",
   "metadata": {},
   "source": [
    "# 5.2 SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34eb5be",
   "metadata": {},
   "source": [
    "Separate points using a  (p−1)\n",
    "  dimensional hyperplane. This means that the SVM will construct a decision boundary such that points on the left are assigned a label of  A\n",
    "  and points on the right are assigned a label of  B\n",
    " . When finding this separating hyperplane we wish to maximise the distance of the nearest points to the hyperplane. The technical term for this is maximum separating hyperplane. The data points which dictate where the separating hyperplane goes are called support vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0329f2ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\percy\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.7177623261694058\n",
      "f1_score 0.7155944011875718\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.63      0.51      0.57       278\n",
      "           0       0.50      0.48      0.49       425\n",
      "           1       0.79      0.80      0.79      1755\n",
      "           2       0.70      0.74      0.72       706\n",
      "\n",
      "    accuracy                           0.72      3164\n",
      "   macro avg       0.65      0.63      0.64      3164\n",
      "weighted avg       0.71      0.72      0.72      3164\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Create a pipeline and make predictions of the bag of words using linearSVC\n",
    "from sklearn.svm import LinearSVC #Import LinearSVC from the sklearn\n",
    "\n",
    "\n",
    "clf= Pipeline([('cf', cf),('clf',  LinearSVC())]) #Create a pipeline with the bag or words features and the linearSVC\n",
    "\n",
    "clf.fit(X_train, y_train) #Fit the training data to the pipeline\n",
    "\n",
    "y_pred = clf.predict(X_test) #Make predictions with the test data\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test)) #Print out the accuracy\n",
    "print('f1_score %s' % f1_score(y_test,y_pred,average='weighted')) #Print out the f1 score\n",
    "print(classification_report(y_test, y_pred)) #Print out the classification repor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75747fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b1917045",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\percy\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.7354614412136536\n",
      "f1_score 0.7266710333792864\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.72      0.50      0.59       278\n",
      "           0       0.55      0.40      0.47       425\n",
      "           1       0.77      0.85      0.81      1755\n",
      "           2       0.72      0.75      0.74       706\n",
      "\n",
      "    accuracy                           0.74      3164\n",
      "   macro avg       0.69      0.62      0.65      3164\n",
      "weighted avg       0.73      0.74      0.73      3164\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## create a pipeline and fit it with a Linear Support Vector Classifier\n",
    "from sklearn.svm import LinearSVC #Import LinearSVC from sklearn \n",
    "\n",
    "classifier = LinearSVC() #Call LinearSVC and assign the variable 'classifier'\n",
    "\n",
    "clf = Pipeline([('tfidf', tfidf), ('clf', classifier)]) #Create a pipeline with the tdidf\n",
    "\n",
    "clf.fit(X_train, y_train) #Fit the model\n",
    "y_pred = clf.predict(X_test) #Make predictions and assign the variable 'y_pred'\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test)) #Print the accuracy\n",
    "print('f1_score %s' % f1_score(y_test,y_pred,average='weighted')) #Print the f1-score\n",
    "print(classification_report(y_test, y_pred)) #Print the classification report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57bc2b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "NAIVE BAYES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0798241d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.8095\n",
      "f1_score 0.8052602891777257\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.85      0.93      0.89       977\n",
      "           0       0.83      0.78      0.80      1024\n",
      "           1       0.79      0.63      0.70       995\n",
      "           2       0.77      0.90      0.83      1004\n",
      "\n",
      "    accuracy                           0.81      4000\n",
      "   macro avg       0.81      0.81      0.81      4000\n",
      "weighted avg       0.81      0.81      0.81      4000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import  BernoulliNB\n",
    "naive_bayes =  BernoulliNB()\n",
    "cnb= Pipeline([('cf', cf),('cnb',  naive_bayes)]) #Create a pipeline with the bag or words features and the linearSVC\n",
    "\n",
    "cnb.fit(X_train, y_train) #Fit the training data to the pipeline\n",
    "\n",
    "y_pred = cnb.predict(X_test) #Make predictions with the test data\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test)) #Print the accuracy\n",
    "print('f1_score %s' % f1_score(y_test,y_pred,average='weighted')) #Print the f1-score\n",
    "print(classification_report(y_test, y_pred)) #Print the classification report\n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "23726bc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.5300252844500632\n",
      "f1_score 0.5187294937155361\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.86      0.06      0.12       278\n",
      "           0       0.25      0.66      0.36       425\n",
      "           1       0.67      0.69      0.68      1755\n",
      "           2       0.80      0.24      0.37       706\n",
      "\n",
      "    accuracy                           0.53      3164\n",
      "   macro avg       0.65      0.41      0.38      3164\n",
      "weighted avg       0.66      0.53      0.52      3164\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import resample\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(10)\n",
    "cnb= Pipeline([('cf', cf),('cnb', knn)]) #Create a pipeline with the bag or words features and the linearSVC\n",
    "\n",
    "cnb.fit(X_train, y_train) #Fit the training data to the pipeline\n",
    "\n",
    "y_pred = cnb.predict(X_test) #Make predictions with the test data\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test)) #Print the accuracy\n",
    "print('f1_score %s' % f1_score(y_test,y_pred,average='weighted')) #Print the f1-score\n",
    "print(classification_report(y_test, y_pred)) #Print the classification report\n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7a9840",
   "metadata": {},
   "outputs": [],
   "source": [
    "Imbalances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f301b897",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 1    5000\n",
       " 0    5000\n",
       " 2    5000\n",
       "-1    5000\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df_majority = df_train.copy()[df_train['sentiment'] == 1]\n",
    "df_minority1 = df_train.copy()[df_train['sentiment'] == 0]\n",
    "df_minority2 = df_train.copy()[df_train['sentiment'] == 2]\n",
    "df_minority3 = df_train.copy()[df_train['sentiment'] == -1]\n",
    "\n",
    "\n",
    "# Downsample majority class\n",
    "df_majority_downsampled = resample(df_majority, \n",
    "                                 replace=False,    # sample without replacement\n",
    "                                 n_samples=5000,     # Using a benchmark of 3640\n",
    "                                 random_state=123) # reproducible results\n",
    "#Upsampling the least minority class\n",
    "df_minority_up = resample(df_minority1, \n",
    "                        replace=True,    # sample without replacement\n",
    "                        n_samples=5000,     # to match the second majority class\n",
    "                        random_state=123) # reproducible results\n",
    "\n",
    "df_minority_up1 = resample(df_minority2, \n",
    "                        replace=True,    # sample without replacement\n",
    "                        n_samples=5000,     # to match the second majority class\n",
    "                        random_state=123) # reproducible results\n",
    "\n",
    "df_minority_up2 = resample(df_minority3, \n",
    "                        replace=True,    # sample without replacement\n",
    "                        n_samples=5000,     # to match the second majority class\n",
    "                        random_state=123) # reproducible results\n",
    "\n",
    "# Combine minority class with downsampled majority class\n",
    "df_resampled = pd.concat([df_majority_downsampled,df_minority_up,df_minority_up1, df_minority_up2])\n",
    " \n",
    "# Display new class counts\n",
    "df_resampled.sentiment.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c84ef64f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16000,)\n",
      "(4000,)\n",
      "(16000,)\n",
      "(4000,)\n"
     ]
    }
   ],
   "source": [
    "X = df_resampled['message']\n",
    "y = df_resampled['sentiment']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ac2d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Resampled Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "50263158",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\percy\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.8765\n",
      "f1_score 0.8738728438701414\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.92      0.98      0.95       977\n",
      "           0       0.87      0.90      0.89      1024\n",
      "           1       0.84      0.71      0.77       995\n",
      "           2       0.87      0.91      0.89      1004\n",
      "\n",
      "    accuracy                           0.88      4000\n",
      "   macro avg       0.87      0.88      0.87      4000\n",
      "weighted avg       0.87      0.88      0.87      4000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression #Import logistic regression model from sklearn\n",
    "\n",
    "model = LogisticRegression(C=100,multi_class='ovr') #Call logistic regression model and assign variable 'model'\n",
    "\n",
    "clf_sam1 = Pipeline([('cf', cf), ('clf', model)]) #Create a pipeline with the logistic model and bag-of-words\n",
    "\n",
    "\n",
    "clf_sam1.fit(X_train, y_train) #Fit the training set\n",
    "\n",
    "y_pred= clf_sam1.predict(X_test) #Fit the test set\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test)) #Print accuracy\n",
    "print('f1_score %s' % f1_score(y_test,y_pred,average='weighted')) #Print f1 score\n",
    "print(classification_report(y_test, y_pred)) #Print classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8ba532b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\percy\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.88625\n",
      "f1_score 0.8841513138681816\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.93      0.98      0.95       977\n",
      "           0       0.89      0.91      0.90      1024\n",
      "           1       0.85      0.74      0.79       995\n",
      "           2       0.87      0.92      0.89      1004\n",
      "\n",
      "    accuracy                           0.89      4000\n",
      "   macro avg       0.88      0.89      0.88      4000\n",
      "weighted avg       0.88      0.89      0.88      4000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create a pipeline and fit it with a Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression #Import logistic regression model from sklearn\n",
    "\n",
    "model = LogisticRegression(C=50,multi_class='ovr') #Call logistic regression model and assign variable 'model'\n",
    "\n",
    "clf_sam = Pipeline([('tfidf', tfidf), ('clf', model)]) #Create a pipeline with the logistic model and tf-idf vectorizer\n",
    "\n",
    "\n",
    "clf_sam.fit(X_train, y_train) #Fit the training set\n",
    "\n",
    "y_pred= clf_sam.predict(X_test) #Fit the test set\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test)) #Print accuracy\n",
    "print('f1_score %s' % f1_score(y_test,y_pred,average='weighted')) #Print f1 score\n",
    "print(classification_report(y_test, y_pred)) #Print classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f18af58",
   "metadata": {},
   "outputs": [],
   "source": [
    "SVC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6866b3d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\percy\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.877\n",
      "f1_score 0.8742687222125705\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.93      0.98      0.95       977\n",
      "           0       0.86      0.91      0.89      1024\n",
      "           1       0.85      0.71      0.77       995\n",
      "           2       0.87      0.91      0.89      1004\n",
      "\n",
      "    accuracy                           0.88      4000\n",
      "   macro avg       0.88      0.88      0.87      4000\n",
      "weighted avg       0.88      0.88      0.87      4000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\percy\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC #Import LinearSVC from the sklearn\n",
    "\n",
    "\n",
    "clf= Pipeline([('cf', cf),('clf',  LinearSVC())]) #Create a pipeline with the bag or words features and the linearSVC\n",
    "\n",
    "clf.fit(X_train, y_train) #Fit the training data to the pipeline\n",
    "\n",
    "y_pred = clf.predict(X_test) #Make predictions with the test data\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test)) #Print out the accuracy\n",
    "print('f1_score %s' % f1_score(y_test,y_pred,average='weighted')) #Print out the f1 score\n",
    "print(classification_report(y_test, y_pred)) #Print out the classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d4847315",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\percy\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.882\n",
      "f1_score 0.8795882805318579\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.92      0.98      0.95       977\n",
      "           0       0.89      0.91      0.90      1024\n",
      "           1       0.85      0.73      0.78       995\n",
      "           2       0.86      0.92      0.89      1004\n",
      "\n",
      "    accuracy                           0.88      4000\n",
      "   macro avg       0.88      0.88      0.88      4000\n",
      "weighted avg       0.88      0.88      0.88      4000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC #Import LinearSVC from sklearn \n",
    "\n",
    "classifier = LinearSVC() #Call LinearSVC and assign the variable 'classifier'\n",
    "\n",
    "clf = Pipeline([('tfidf', tfidf), ('clf', classifier)]) #Create a pipeline with the tdidf\n",
    "\n",
    "clf.fit(X_train, y_train) #Fit the model\n",
    "y_pred = clf.predict(X_test) #Make predictions and assign the variable 'y_pred'\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test)) #Print the accuracy\n",
    "print('f1_score %s' % f1_score(y_test,y_pred,average='weighted')) #Print the f1-score\n",
    "print(classification_report(y_test, y_pred)) #Print the classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2badbb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "46343e67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.8095\n",
      "f1_score 0.8052602891777257\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.85      0.93      0.89       977\n",
      "           0       0.83      0.78      0.80      1024\n",
      "           1       0.79      0.63      0.70       995\n",
      "           2       0.77      0.90      0.83      1004\n",
      "\n",
      "    accuracy                           0.81      4000\n",
      "   macro avg       0.81      0.81      0.81      4000\n",
      "weighted avg       0.81      0.81      0.81      4000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "naive_bayes =  BernoulliNB()\n",
    "cnb= Pipeline([('cf', cf),('cnb',  naive_bayes)]) #Create a pipeline with the bag or words features and the linearSVC\n",
    "\n",
    "cnb.fit(X_train, y_train) #Fit the training data to the pipeline\n",
    "\n",
    "y_pred = cnb.predict(X_test) #Make predictions with the test data\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test)) #Print the accuracy\n",
    "print('f1_score %s' % f1_score(y_test,y_pred,average='weighted')) #Print the f1-score\n",
    "print(classification_report(y_test, y_pred)) #Print the classification report\n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c04eefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "KNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "cb274864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.5515\n",
      "f1_score 0.5307405587883788\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.77      0.68      0.72       977\n",
      "           0       0.39      0.95      0.56      1024\n",
      "           1       0.82      0.22      0.34       995\n",
      "           2       0.87      0.36      0.51      1004\n",
      "\n",
      "    accuracy                           0.55      4000\n",
      "   macro avg       0.71      0.55      0.53      4000\n",
      "weighted avg       0.71      0.55      0.53      4000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import resample\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(10)\n",
    "cnb= Pipeline([('cf', cf),('cnb', knn)]) #Create a pipeline with the bag or words features and the linearSVC\n",
    "\n",
    "cnb.fit(X_train, y_train) #Fit the training data to the pipeline\n",
    "\n",
    "y_pred = cnb.predict(X_test) #Make predictions with the test data\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test)) #Print the accuracy\n",
    "print('f1_score %s' % f1_score(y_test,y_pred,average='weighted')) #Print the f1-score\n",
    "print(classification_report(y_test, y_pred)) #Print the classification report\n",
    "               "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0159464",
   "metadata": {},
   "source": [
    "<a id=\"six\"></a>\n",
    "\n",
    "## 6. Model Performance\n",
    "\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "\n",
    "| ⚡ Description: Model performance ⚡                                                                                                                                      |\n",
    "| :------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |\n",
    "| In this section you are required to compare the relative performance of the various trained ML models on a holdout dataset and comment on what model is the best and why. |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9194b7",
   "metadata": {},
   "source": [
    "<a id=\"seven\"></a>\n",
    "\n",
    "## 7. Model Explanations\n",
    "\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "\n",
    "| ⚡ Description: Model explanation ⚡                                                                                                                                                                              |\n",
    "| :---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| In this section, you are required to discuss how the best performing model works in a simple way so that both technical and non-technical stakeholders can grasp the intuition behind the model's inner workings. |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c25acd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
