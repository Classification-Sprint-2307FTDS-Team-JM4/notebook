{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63e77ef7",
   "metadata": {},
   "source": [
    "# ADVANCED CLASSIFICATION PREDICT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2119e48",
   "metadata": {},
   "source": [
    "# Predict Overview: EA Twitter Sentiment Classification\n",
    "Companies would like to determine how people perceive climate change and whether or not they believe it is a real threat. \n",
    "\n",
    "Our mission is to deliver a precise and durable solution to this objective, granting companies the ability to tap into a wide range of consumer sentiments across various demographics and geographic regions. This, in turn, enhances their understanding and empowers them to shape future marketing strategies based on valuable insights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc67ce0",
   "metadata": {},
   "source": [
    "<a id=\"cont\"></a>\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "<a href=#one>1. Importing Packages</a>\n",
    "\n",
    "<a href=#two>2. Loading Data</a>\n",
    "\n",
    "<a href=#three>3. Exploratory Data Analysis (EDA)</a>\n",
    "\n",
    "<a href=#four>4. Data Engineering</a>\n",
    "\n",
    "<a href=#five>5. Modeling</a>\n",
    "\n",
    "<a href=#six>6. Model Performance</a>\n",
    "\n",
    "<a href=#seven>7. Model Explanations</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c3c4e1",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Dataset Description\n",
    "\n",
    "Where is this data from?\n",
    "- The collection of this data was funded by a Canada Foundation for Innovation JELF Grant to Chris Bauch, University of Waterloo. The dataset aggregates tweets pertaining to climate change collected between Apr 27, 2015 and Feb 21, 2018. In total, 43,943 tweets were collected. Each tweet is labelled as one of 4 classes, which are described below.\n",
    "\n",
    "Class Description\n",
    "\n",
    "- 2 News: the tweet links to factual news about climate change\n",
    "\n",
    "- 1 Pro: the tweet supports the belief of man-made climate change\n",
    "\n",
    "- 0 Neutral: the tweet neither supports nor refutes the belief of man-made climate change\n",
    "\n",
    "- -1 Anti: the tweet does not believe in man-made climate change Variable definitions\n",
    "\n",
    "Features\n",
    "\n",
    "sentiment: Which class a tweet belongs in (refer to Class Description above)\n",
    "\n",
    "- message: Tweet body\n",
    "\n",
    "- tweetid: Twitter unique id\n",
    "\n",
    "The files provided\n",
    "\n",
    "train.csv - You will use this data to train your model.\n",
    "\n",
    "test.csv - You will use this data to test your model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74afed89",
   "metadata": {},
   "source": [
    "<a id=\"one\"></a>\n",
    "\n",
    "## 1. Importing Packages\n",
    "\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "\n",
    "| ⚡ Description: Importing Packages ⚡                                                                                                    |\n",
    "| :--------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| In this section you are required to import, and briefly discuss, the libraries that will be used throughout your analysis and modelling. |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6b3668",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.style as style\n",
    "%matplotlib inline\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "nltk.download('stopwords')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer \n",
    "nltk.download('vader_lexicon')\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression #Import Logistic Regression from the sklearn\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from wordcloud import WordCloud,ImageColorGenerator\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from sklearn.feature_extraction.text import CountVectorizer #Import CountVectorizer from sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1668cac0",
   "metadata": {},
   "source": [
    "<a id=\"two\"></a>\n",
    "\n",
    "## 2. Loading the Data\n",
    "\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "\n",
    "| ⚡ Description: Loading the data ⚡                                                          |\n",
    "| :------------------------------------------------------------------------------------------- |\n",
    "| In this section you are required to load the data from the `df_train` file into a DataFrame. |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2e01b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv (r\"C:\\Users\\percy\\Downloads\\Advanced_Classification_Predict-student_data-2780\\test_with_no_labels.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f84e83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(r\"C:\\Users\\percy\\Downloads\\Advanced_Classification_Predict-student_data-2780\\train.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf2214d",
   "metadata": {},
   "source": [
    "data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ec6305",
   "metadata": {},
   "source": [
    "<a id=\"three\"></a>\n",
    "\n",
    "## 3. Exploratory Data Analysis (EDA)\n",
    "\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "\n",
    "| ⚡ Description: Exploratory data analysis ⚡                                                             |\n",
    "| :------------------------------------------------------------------------------------------------------- |\n",
    "| In this section, you are required to perform an in-depth analysis of all the variables in the DataFrame. |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d743c549",
   "metadata": {},
   "source": [
    "The EDA  includes:\n",
    "- Summary analysis\n",
    "- Null values\n",
    "- Classes of tweets - Sentiments\n",
    "- Number of words of tweets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c60ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84aa0fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0cb5a8",
   "metadata": {},
   "source": [
    "Summary analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799659a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ee524c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9542d4",
   "metadata": {},
   "source": [
    "Null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a5c6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77ab204",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no null values in the test and train data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ckecking the number of rows and columns in our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1ad5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81c3bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There are 3 columns and 15819 rows in train data\n",
    "- There are 2 coluns and 10546 rows in test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c58cea",
   "metadata": {},
   "source": [
    "Sentiments meaning as mentioned in the introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823096f6",
   "metadata": {},
   "source": [
    "- Negative climate Change = -1\n",
    "- Neutral = 0\n",
    "- Positive Climate change = 1\n",
    "- News = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0534bba",
   "metadata": {},
   "source": [
    "Comparing the number of tweets of sentiments\n",
    "- Distribution\n",
    "- Pie chart\n",
    "- Word cloud\n",
    "- Top 20 words used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9577156",
   "metadata": {},
   "source": [
    "Bar chart showing the counts of tweets per sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed427a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "senti_counts = df_train[\"sentiment\"].value_counts()\n",
    "news = senti_counts[2] \n",
    "pro = senti_counts[1]   \n",
    "neutral = senti_counts[0]\n",
    "anti = senti_counts[-1]  \n",
    "\n",
    "plt.figure( figsize=(9,4))\n",
    "plt.barh(['News ','Pro','Neutral','Anti'], [news,pro,neutral,anti]) # horizontal bar graph to compare classes of tweets.\n",
    "plt.colours = ['red', 'green', 'blue', 'orange']\n",
    "plt.xlabel('Count of Tweets') #X-label of the data\n",
    "plt.ylabel('Sentiment Classification') #Y_label of the data \n",
    "plt.title('Distribution of Classes In The Dataset') #Give the data a title 'Dataset lables distribution'\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ee3b9e",
   "metadata": {},
   "source": [
    "Pie chart showing the percentages of tweets per sentiment - Which one has the highest proportion between news and pro climate change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dac3562",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_counts = df_train[\"sentiment\"].value_counts()\n",
    "plt.figure( figsize=(9,4))\n",
    "plt.pie(class_counts, labels=['Pro ','News','Neutral','Anti'], explode=[0.05,0.05,0.05,0.05], autopct='%1.1f%%')\n",
    "plt.title(\"Pie Chart of Percentage classification of Tweets \")\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64cf6484",
   "metadata": {},
   "source": [
    "Insights:\n",
    "- News has the highest percentage of tweets followed by Pro climate change, Neutral and anti climate change respectively"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"four\"></a>\n",
    "\n",
    "## 4. Feature Engineering\n",
    "\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "\n",
    "| ⚡ Description Feature engineering ⚡                                                                                        |\n",
    "| :------------------------------------------------------------------------------------------------------------------------- |\n",
    "| In this section we cleaned the data, added new features- as identified in the EDA phase. |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 4.1 Adding and removing columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding Length tweets column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = [len(tweet) for tweet in df_train['message']]\n",
    "df_train['length'] = length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the *length of the tweet* per categories\n",
    "- Box plots\n",
    "- Distribution bar graphs\n",
    "- Summary of Stats\n",
    "- Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Box plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating class subsets for the datase\n",
    "df_anti = df_train.copy()[df_train['sentiment'] == -1]\n",
    "df_neutral = df_train.copy()[df_train['sentiment'] == 0]\n",
    "df_pro = df_train.copy()[df_train['sentiment'] == 1]\n",
    "df_news = df_train.copy()[df_train['sentiment'] == 2]\n",
    "\n",
    "#storing the size data in separate variables\n",
    "pro_len = df_pro['length']\n",
    "neutral_len = df_neutral['length']\n",
    "anti_len = df_anti['length']\n",
    "news_len = df_news['length']\n",
    "data_len = df_train['length']\n",
    "\n",
    "#creating a list of all the length datasets\n",
    "len_data = [pro_len, anti_len, neutral_len, news_len, data_len]\n",
    "\n",
    "# Create a figure and axis\n",
    "fig, ax = plt.subplots(figsize=(9,4))\n",
    "\n",
    "# Create the box plots\n",
    "ax.boxplot(len_data, vert=False)\n",
    "\n",
    "# Set the labels for each box plot\n",
    "labels = ['pro', 'anti', 'neutral', 'news', 'main data']\n",
    "ax.set_yticklabels(labels)\n",
    "\n",
    "# Set the title and axis labels\n",
    "plt.title('Box and Whiskers Diagram For Tweet Lengths Per Category')\n",
    "plt.xlabel('Length In Characters')\n",
    "plt.ylabel('Class of Tweet')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('mean of pro-climate is:  ', round(df_pro['length'].mean(),2))\n",
    "round(df_pro['length'].describe(),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('mean of anti-climate is:  ', round(df_anti['length'].mean(),2))\n",
    "round(df_anti['length'].describe(),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('mean of neutral-climate is:  ', round(df_neutral['length'].mean(),2))\n",
    "round(df_neutral['length'].describe(),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('mean of news is:  ', round(df_news['length'].mean(),2))\n",
    "round(df_news['length'].describe(),2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pro tweets length have the highest mean followed by anti, news and neutral respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bar graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure( figsize=(9,4))\n",
    "plt.hist(df_train['length'])\n",
    "plt.title(\"Distribution of Tweet Lengths\")\n",
    "plt.xlabel(\"Length of Tweet In Charaters\") #X-label of the data\n",
    "plt.ylabel(\"Number of Tweets\")      #Y_label of the data\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The highest number of tweets lie between the 125 - 150 length of characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation between length of tweets and sentiment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the correlation matrix\n",
    "correlation_matrix = df_train.corr()\n",
    "\n",
    "# Calculate the correlations with the target variable\n",
    "target_correlations = correlation_matrix['sentiment']\n",
    "\n",
    "# Sort the features by their correlations with the target variable\n",
    "sorted_features = target_correlations.abs().sort_values(ascending=False).index\n",
    "\n",
    "# Reorder the correlation matrix\n",
    "sorted_corr_matrix = correlation_matrix.loc[sorted_features, sorted_features]\n",
    "\n",
    "# Create a heatmap for the sorted correlations without annotations\n",
    "plt.figure(figsize=(7, 5))\n",
    "sns.heatmap(sorted_corr_matrix, cmap='coolwarm')\n",
    "plt.title('Correlation with sentiments')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no correlation between the tweet id, length and sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping tweet Id "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reason: Tweet Id has no significance in our analysis as it is only a unique number of the tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.drop('tweetid', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Data cleaning \n",
    "In order to process the tweet messages more effectively the tweets are cleaned using the clean function defined in the code cell below. The clean function does the following.\n",
    "\n",
    "- Remove urls\n",
    "- Convert all tweet text to lowercase.\n",
    "- Remove punctuation, numbers & emojis\n",
    "- Remove stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing the web-urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern_url = r'http[s]?://(?:[A-Za-z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9A-Fa-f][0-9A-Fa-f]))+'\n",
    "subs_url = r'url-web'\n",
    "df_train['CleanMessage'] = df_train['message'].replace(to_replace = pattern_url, value = subs_url, regex = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting every word to lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['CleanMessage'] = df_train['CleanMessage'].str.lower()\n",
    "df_train['CleanMessage'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "removing punctuations, numbers and emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(post):\n",
    "    return ''.join([l for l in post if l not in string.punctuation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['CleanMessage'] = df_train['CleanMessage'].apply(remove_punctuation)\n",
    "df_train['CleanMessage'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['CleanMessage'] = df_train['CleanMessage'].astype(str).apply(lambda x: re.sub(\"[^a-z]\", \" \", x))\n",
    "\n",
    "df_train['CleanMessage'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwordlist = ['a', 'about', 'above', 'after', 'again', 'ain', 'all', 'am', 'an',\n",
    "             'and','any','are', 'as', 'at', 'be', 'because', 'been', 'before',\n",
    "             'being', 'below', 'between','both', 'by', 'can', 'd', 'did', 'do',\n",
    "             'does', 'doing', 'down', 'during', 'each','few', 'for', 'from',\n",
    "             'further', 'had', 'has', 'have', 'having', 'he', 'her', 'here',\n",
    "             'hers', 'herself', 'him', 'himself', 'his', 'how', 'i', 'if', 'in',\n",
    "             'into','is', 'it', 'its', 'itself', 'just', 'll', 'm', 'ma',\n",
    "             'me', 'more', 'most','my', 'myself', 'now', 'o', 'of', 'on', 'once',\n",
    "             'only', 'or', 'other', 'our', 'ours','ourselves', 'out', 'own', 're','s', 'same', 'she', \"shes\", 'should', \"shouldve\",'so', 'some', 'such',\n",
    "             't', 'than', 'that', \"thatll\", 'the', 'their', 'theirs', 'them',\n",
    "             'themselves', 'then', 'there', 'these', 'they', 'this', 'those',\n",
    "             'through', 'to', 'too','under', 'until', 'up', 've', 'very', 'was',\n",
    "             'we', 'were', 'what', 'when', 'where','which','while', 'who', 'whom',\n",
    "             'why', 'will', 'with', 'won', 'y', 'you', \"youd\",\"youll\", \"youre\",\n",
    "             \"youve\", 'your', 'yours', 'yourself', 'yourselves']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS = set(stopwordlist)\n",
    "def cleaning_stopwords(text):\n",
    "    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n",
    "df_train['CleanMessage'] = df_train['CleanMessage'].apply(lambda text: cleaning_stopwords(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1a292c",
   "metadata": {},
   "source": [
    "Word cloud showing top words used in every class in the cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d30320f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "text = \" \".join(i for i in df_train['CleanMessage'])\n",
    "text = str(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b81c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud()\n",
    "tweet_cloud = wordcloud.generate(text)\n",
    "plt.figure( figsize=(9,4))\n",
    "plt.imshow(tweet_cloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Climate change and Global warming are the two most tweeted words in our data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2555054f",
   "metadata": {},
   "source": [
    "Top 20 words in the tweets cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070215e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_dict = {}\n",
    "words = text.split(\" \")\n",
    "for word in words:\n",
    "    if word != \" \" and word !=\"\":\n",
    "        if word not in freq_dict:\n",
    "            freq_dict[word] = 1\n",
    "        else:\n",
    "            freq_dict[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccffaad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_freq_dict = sorted(freq_dict.items(), key=lambda x:x[1], reverse=True)[:20]\n",
    "top_10_words = dict(sorted_freq_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d66e550",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_labels = list(top_10_words.keys())\n",
    "values = list(top_10_words.values())\n",
    "\n",
    "# Create the figure and axes\n",
    "fig, ax = plt.subplots(figsize=(9,4))\n",
    "\n",
    "# Plot the data\n",
    "ax.bar(x_labels, values)\n",
    "\n",
    "# Customize the plot\n",
    "ax.set_xlabel('word')\n",
    "ax.set_ylabel('Word Count')\n",
    "ax.set_title('Top 20 Most Used Words')\n",
    "\n",
    "# Rotate the x-labels if needed\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de04458f",
   "metadata": {},
   "source": [
    "The top of words used\n",
    "- 1. Climate\n",
    "- 2. Change\n",
    "- 3. rt\n",
    "- 4. urlweb\n",
    "- 5. Global\n",
    "- 6. Warming\n",
    "- 7. trump\n",
    "- 8. believe\n",
    "- 9. not\n",
    "- 10. us\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most used words of the anti climate change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating class subsets for the datase\n",
    "df_anti = df_train.copy()[df_train['sentiment'] == -1]\n",
    "df_neutral = df_train.copy()[df_train['sentiment'] == 0]\n",
    "df_pro = df_train.copy()[df_train['sentiment'] == 1]\n",
    "df_news = df_train.copy()[df_train['sentiment'] == 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf68079",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "text = \" \".join(i for i in df_anti[\"CleanMessage\"])\n",
    "text = str(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2f1d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_dict = {}\n",
    "words = text.split(\" \")\n",
    "for word in words:\n",
    "    if word != \" \" and word !=\"\":\n",
    "        if word not in freq_dict:\n",
    "            freq_dict[word] = 1\n",
    "        else:\n",
    "            freq_dict[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d46c34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_freq_dict = sorted(freq_dict.items(), key=lambda x:x[1], reverse=True)[:20]\n",
    "top_10_words = dict(sorted_freq_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061a2372",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_labels = list(top_10_words.keys())\n",
    "values = list(top_10_words.values())\n",
    "\n",
    "# Create the figure and axes\n",
    "fig, ax = plt.subplots(figsize=(9,4))\n",
    "\n",
    "# Plot the data\n",
    "ax.bar(x_labels, values)\n",
    "\n",
    "# Customize the plot\n",
    "ax.set_xlabel('word')\n",
    "ax.set_ylabel('Word Count')\n",
    "ax.set_title('Top 20 Most Used Words')\n",
    "\n",
    "# Rotate the x-labels if needed\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top of words used for anti\n",
    "- 1. Climate\n",
    "- 2. Change\n",
    "- 3. rt\n",
    "- 4. Global\n",
    "- 5. Warming\n",
    "- 6. urlweb\n",
    "- 7. not\n",
    "- 8. no\n",
    "- 9. Global\n",
    "- 10. Warming\n",
    "- 11. trump\n",
    "- 12. believe\n",
    "- 13. not\n",
    "- 14. us\n",
    "- 15. real\n",
    "- 16. People\n",
    "- 17. man\n",
    "- 18. scam\n",
    "- 19. would\n",
    "- 20. fake \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \" \".join(i for i in df_pro[\"CleanMessage\"])\n",
    "text = str(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_dict = {}\n",
    "words = text.split(\" \")\n",
    "for word in words:\n",
    "    if word != \" \" and word !=\"\":\n",
    "        if word not in freq_dict:\n",
    "            freq_dict[word] = 1\n",
    "        else:\n",
    "            freq_dict[word] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_freq_dict = sorted(freq_dict.items(), key=lambda x:x[1], reverse=True)[:20]\n",
    "top_10_words = dict(sorted_freq_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_labels = list(top_10_words.keys())\n",
    "values = list(top_10_words.values())\n",
    "\n",
    "# Create the figure and axes\n",
    "fig, ax = plt.subplots(figsize=(9,4))\n",
    "\n",
    "# Plot the data\n",
    "ax.bar(x_labels, values)\n",
    "\n",
    "# Customize the plot\n",
    "ax.set_xlabel('word')\n",
    "ax.set_ylabel('Word Count')\n",
    "ax.set_title('Top 10 Most Used Words')\n",
    "\n",
    "# Rotate the x-labels if needed\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top of words used for pro\n",
    "- 1. Climate\n",
    "- 2. Change\n",
    "- 3. rt\n",
    "- 4. urlweb\n",
    "- 5. Global\n",
    "- 6. Warming\n",
    "- 7. believe\n",
    "- 8. trump\n",
    "- 9. doesnt\n",
    "- 10. amp\n",
    "- 11. not\n",
    "- 12. going\n",
    "- 13. but\n",
    "- 14. real\n",
    "- 15. people\n",
    "- 16. real\n",
    "- 17. us\n",
    "- 18. die\n",
    "- 19. dont\n",
    "- 20. thinking\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a658d945",
   "metadata": {},
   "source": [
    "Cleaning also included:\n",
    "- Tokenization\n",
    "- Stemming\n",
    "- Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74834b56",
   "metadata": {},
   "source": [
    "Applying Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3631c8",
   "metadata": {},
   "source": [
    "A tokeniser divides text into a sequence of tokens, which roughly correspond to \"words\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58ecddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, TreebankWordTokenizer\n",
    "tokeniser = TreebankWordTokenizer()\n",
    "df_train['tokens'] = df_train['CleanMessage'].apply(tokeniser.tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454a2c6b",
   "metadata": {},
   "source": [
    "Applying stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c8f082",
   "metadata": {},
   "source": [
    "Stemming is the process of transforming to the root word. It uses an algorithm that removes common word-endings from English words, such as “ly,” “es,” “ed,” and “s.”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b4becb",
   "metadata": {},
   "outputs": [],
   "source": [
    "st = nltk.PorterStemmer()\n",
    "def stemming_on_text(data):\n",
    "    text = [st.stem(word) for word in data]\n",
    "    return data\n",
    "df_train['CleanMessage']= df_train['CleanMessage'].apply(lambda x: stemming_on_text(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58a33ee",
   "metadata": {},
   "source": [
    "Applying lemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4c2282",
   "metadata": {},
   "source": [
    "Lemmatizing is the process of grouping words of similar meaning together. So, your root stem, meaning the word you end up with, is not something you can just look up in a dictionary, but you can look up a lemma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63016553",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = nltk.WordNetLemmatizer()\n",
    "def lemmatizer_on_text(data):\n",
    "    text = [lm.lemmatize(word) for word in data]\n",
    "    return data\n",
    "df_train['CleanMessage'] = df_train['CleanMessage'].apply(lambda x: lemmatizer_on_text(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172b63ad",
   "metadata": {},
   "source": [
    "4.3 Text Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f38399",
   "metadata": {},
   "source": [
    "Text feature extraction is the process of transforming what is essentially a list of words into a feature set that is usable by a classifier. The NLTK classifiers expect dict style feature sets, so we must therefore transform our text into a dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We added the following features to modify our and for more insights:\n",
    "- Bag of words\n",
    "- TFIDVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310d3876",
   "metadata": {},
   "source": [
    "Bag of words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6eeb5ee",
   "metadata": {},
   "source": [
    "The Bag of Words model is the simplest method; it constructs a word presence feature set from all the words in the text, indicating the number of times each word has appeared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad334f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words_count(words, word_dict={}):\n",
    "    \"\"\" this function takes in a list of words and returns a dictionary \n",
    "        with each word as a key, and the value represents the number of \n",
    "        times that word appeared\"\"\"\n",
    "    for word in words:\n",
    "        if word in word_dict.keys():\n",
    "            word_dict[word] += 1\n",
    "        else:\n",
    "            word_dict[word] = 1\n",
    "    return word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30fa461f",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_sentiment = list(df_train[\"sentiment\"].unique())\n",
    "print(unique_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7fd970",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dce7062",
   "metadata": {},
   "outputs": [],
   "source": [
    "personality = {}\n",
    "for pp in unique_sentiment:\n",
    "    df = df_train.groupby('sentiment')\n",
    "    personality[pp] = {}\n",
    "    for row in df.get_group(pp)['tokens']:\n",
    "        personality[pp] = bag_of_words_count(row, personality[pp])       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be549dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = set()\n",
    "for pp in unique_sentiment:\n",
    "    for word in personality[pp]:\n",
    "        all_words.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a837a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "personality['all'] = {}\n",
    "for pp in unique_sentiment:    \n",
    "    for word in all_words:\n",
    "        if word in personality[pp].keys():\n",
    "            if word in personality['all']:\n",
    "                personality['all'][word] += personality[pp][word]\n",
    "            else:\n",
    "                personality['all'][word] = personality[pp][word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573caf82",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_words = sum([v for v in personality['all'].values()])\n",
    "total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ee4bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = plt.hist([v for v in personality['all'].values() if v < 10],bins=8)\n",
    "plt.ylabel(\"# of words\")\n",
    "plt.xlabel(\"word frequency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2f5369",
   "metadata": {},
   "outputs": [],
   "source": [
    "len([v for v in personality['all'].values() if v == 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03daa153",
   "metadata": {},
   "outputs": [],
   "source": [
    "len([v for v in personality['all'].values() if v == 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking for rare words in our data\n",
    "- that occurs once\n",
    "- that occurs less than 10 times\n",
    "- that occur 100 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d05a73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rare_words = [k for k, v in personality['all'].items() if v==1] \n",
    "print(rare_words[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124c8abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Words that appear more than 10 times \n",
    "com_words = [k for k, v in personality['all'].items() if v>10] \n",
    "print(rare_words[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20cfa30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many words appear more than 10 times?\n",
    "print(len([v for v in personality['all'].values() if v >= 10]))\n",
    "# how many words of the total does that account for?\n",
    "occurs_more_than_10_times = sum([v for v in personality['all'].values() if v >= 10])\n",
    "print(occurs_more_than_10_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4e5a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_count = 10\n",
    "remaining_word_index = [k for k, v in personality['all'].items() if v > max_count]\n",
    "remaining_word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c728919",
   "metadata": {},
   "source": [
    "Term Frequency – Inverse Document Frequency (TF – IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e41c5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = TfidfVectorizer()\n",
    "corpus = df_train[\"message\"]\n",
    "X = obj.fit_transform(corpus)\n",
    "print (X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbea3a33",
   "metadata": {},
   "source": [
    "<a id=\"five\"></a>\n",
    "\n",
    "## 5. Modelling\n",
    "\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "\n",
    "| ⚡ Description: Modelling ⚡                                                                                                                |\n",
    "| :------------------------------------------------------------------------------------------------------------------------------------------ |\n",
    "| In this section, you are required to create one or more regression models that are able to accurately predict the twitter sentiments. |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62d6c51",
   "metadata": {},
   "source": [
    "### Pre processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932f8cc2",
   "metadata": {},
   "source": [
    "The line X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=50) is splitting the data into training and validation sets.\n",
    "\n",
    "X and y are the feature matrix and target variable, respectively. X contains the features we use to make predictions, and y contains the corresponding target values (in this case, sentiment).\n",
    "\n",
    "train_test_split() is a function provided by scikit-learn (a popular machine learning library) that is used to split a dataset into training and validation subsets.\n",
    "\n",
    "The function takes the following arguments:\n",
    "\n",
    "X: The feature matrix (independent variables). y: The target variable (dependent variable). test_size: This parameter specifies the proportion of the data that should be used for the validation set. In this case, it's set to 0.2, meaning 20% of the data will be used for validation, and the remaining 80% for training. random_state: This is a seed for the random number generator used in the data splitting process. Setting this to a specific value (e.g., 42) ensures that the split is reproducible.\n",
    "\n",
    "train_test_split() returns four sets of data:\n",
    "\n",
    "X_train: This contains the feature data for the training set. X_val: This contains the feature data for the validation set. y_train: This contains the target data for the training set. y_val: This contains the target data for the validation set. By splitting the data into training and validation sets, you can train your machine learning model on a portion of the data (X_train and y_train) and evaluate its performance on another portion that it hasn't seen during training (X_val and y_val). This allows us to estimate how well your model is likely to perform on unseen data (seen in Model Performance section)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42053bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479c040b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1844d08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting features and target variables\n",
    "X = df_train['CleanMessage'] #X is the features of the cleaned tweets\n",
    "y = df_train['sentiment']    #Y is the target variable which is the train sentiment\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42) #Splitting train set into training and testing data\n",
    "#Print out the shape of the training set and the testing set\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6441432c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a step-by-step flow diagram\n",
    "steps = ['Step 1', 'Step 2', 'Step 3', 'Step 4', 'Step 5']\n",
    "actions = ['Clean data', 'BOW and TFID', 'Balance data', 'BOW & TFID', 'Hyperparameter T']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "\n",
    "# Draw rectangles for each step\n",
    "for i, step in enumerate(steps):\n",
    "    rect = mpatches.Rectangle((i, 0), 1, 1, ec='black', fc='White')\n",
    "    ax.add_patch(rect)\n",
    "    ax.text(i + 0.5, 0.5, step, ha='center', va='center', color='black')\n",
    "\n",
    "# Draw arrows between steps\n",
    "for i in range(len(steps) - 1):\n",
    "    ax.arrow(i + 0.5, 0.5, 1, 0, head_width=0.05, head_length=0.05, fc='black', ec='black')\n",
    "\n",
    "# Set axis limits and remove ticks\n",
    "ax.set_xlim(-0.5, len(steps), 1)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "\n",
    "# Add action labels below steps\n",
    "for i, action in enumerate(actions):\n",
    "    ax.text(i + 0.5, -0.2, action, ha='center', va='center', color='black')\n",
    "\n",
    "plt.title('Modelling process')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ed83f6",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 5.A Modelling - Clean data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178066a6",
   "metadata": {},
   "source": [
    "In building a model to classify twitter sentiments for climate change we used the following classifiers on clean data:\n",
    "-  Logistic Regression\n",
    "- Linear & non linear SVC\n",
    "- Decision Trees\n",
    "- Naive Bayes\n",
    "- KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c7062c",
   "metadata": {},
   "source": [
    "The modelling was done using the bag words and Term Frequency – Inverse Document Frequency (TF – IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d9c378",
   "metadata": {},
   "outputs": [],
   "source": [
    "cf = CountVectorizer() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15dda0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling\n",
    "- Scalling reduced the prefomance of our models so we used unscaled data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4739aa4c",
   "metadata": {},
   "source": [
    "### A. Logistic Regression - Bag of words\n",
    "it makes use of a common S-shaped curve known as the logistic function. This curve is commonly known as a sigmoid. It solves the problem for the following reasons:\n",
    "\n",
    "-  It squeezes the range of output values to exist only between 0 and 1.\n",
    "-  It has a point of inflection, which can be used to separate the feature space into two distinct areas (one for each class).\n",
    "-  It has shallow gradients at both its top and bottom, which can be mapped to zeroes or ones respectively with little ambiguity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371e6f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression() #Call logistic regression model and assign variable 'model'\n",
    "\n",
    "text_lr = Pipeline([('cf', cf), ('clf', model)]) #Create a pipeline with the logistic model and tf-idf vectorizer\n",
    "\n",
    "\n",
    "text_lr.fit(X_train, y_train) #Fit the training set\n",
    "\n",
    "y_pred= text_lr.predict(X_test) #Fit the test set\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test)) #Print accuracy\n",
    "print('f1_score %s' % f1_score(y_test,y_pred,average='weighted')) #Print f1 score\n",
    "print(classification_report(y_test, y_pred)) #Print classification report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d186cec3",
   "metadata": {},
   "source": [
    "-  For the logisctic regression precision, recall and f1-score values for the pro and news class are higher.Most tweets falls under pro class and news so the model gets better at classifiying them because it has more evidence of them.\n",
    "\n",
    "- The neutral and negative class are lower.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39fefa6c",
   "metadata": {},
   "source": [
    "## B Linear SVM - Bag of words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f282cee8",
   "metadata": {},
   "source": [
    "Separate points using a  (p−1)\n",
    "  dimensional hyperplane. This means that the SVM will construct a decision boundary such that points on the left are assigned a label of  A\n",
    "  and points on the right are assigned a label of  B\n",
    " . When finding this separating hyperplane we wish to maximise the distance of the nearest points to the hyperplane. The technical term for this is maximum separating hyperplane. The data points which dictate where the separating hyperplane goes are called support vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f6ce35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a pipeline and make predictions of the bag of words using linearSVC\n",
    "from sklearn.svm import LinearSVC #Import LinearSVC from the sklearn\n",
    "\n",
    "\n",
    "clf= Pipeline([('cf', cf),('clf',  LinearSVC())]) #Create a pipeline with the bag or words features and the linearSVC\n",
    "\n",
    "clf.fit(X_train, y_train) #Fit the training data to the pipeline\n",
    "\n",
    "y_pred = clf.predict(X_test) #Make predictions with the test data\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test)) #Print out the accuracy\n",
    "print('f1_score %s' % f1_score(y_test,y_pred,average='weighted')) #Print out the f1 score\n",
    "print(classification_report(y_test, y_pred)) #Print out the classification repor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  C. non Linear SVM - Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a pipeline and make predictions of the bag of words using linearSVC\n",
    "from sklearn.svm import LinearSVC #Import LinearSVC from the sklearn\n",
    "\n",
    "\n",
    "clf= Pipeline([('cf', cf),('clf',  SVC())]) #Create a pipeline with the bag or words features and the linearSVC\n",
    "\n",
    "clf.fit(X_train, y_train) #Fit the training data to the pipeline\n",
    "\n",
    "y_pred = clf.predict(X_test) #Make predictions with the test data\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test)) #Print out the accuracy\n",
    "print('f1_score %s' % f1_score(y_test,y_pred,average='weighted')) #Print out the f1 score\n",
    "print(classification_report(y_test, y_pred)) #Print out the classification repor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9409a8",
   "metadata": {},
   "source": [
    "- Similarly to Logistic regression, The LVC precision, recall and f1-score values for the pro and news class are higher.Most tweets falls under pro class and news so the model gets better at classifiying them because it has more evidence of them.\n",
    "\n",
    "- The neutral and negative class are lower.\n",
    "\n",
    "- The perfomance of LVC is slightly lower that Logisctic regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce990693",
   "metadata": {},
   "source": [
    "### D Decision Trees - Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42913d09",
   "metadata": {},
   "source": [
    "A decision tree is a decision support tool that uses a tree-like graph or model of decisions and their possible consequences. It is one way to display an algorithm that only contains conditional control statements.\n",
    "\n",
    "Decision trees are extremely intuitive ways to classify objects or predict continuous values: you simply ask a series of questions designed to zero-in on the classification/prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ca8447",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "clx= Pipeline([('cf', cf),('clx',  DecisionTreeClassifier(random_state=42))]) #Create a pipeline with the bag or words features and the linearSVC\n",
    "\n",
    "clx.fit(X_train, y_train) #Fit the training data to the pipeline\n",
    "\n",
    "y_pred = clx.predict(X_test) #Make predictions with the test data\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test)) #Print out the accuracy\n",
    "print('f1_score %s' % f1_score(y_test,y_pred,average='weighted')) #Print out the f1 score\n",
    "print(classification_report(y_test, y_pred)) #Print out the classification repor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03a90e3",
   "metadata": {},
   "source": [
    " Similarly to Logistic regression, The Decision Trees model precision, recall and f1-score values for the pro and news class are higher.Most tweets falls under pro class and news so the model gets better at classifiying them because it has more evidence of them.\n",
    "\n",
    "- The neutral and negative class are lower.\n",
    "\n",
    "- The perfomance of Decision Trees is lower that Logisctic regression and LVC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8abffdf",
   "metadata": {},
   "source": [
    "### E. NAIVE BAYES (Bernoulli) - Bag of words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1648770c",
   "metadata": {},
   "source": [
    "Naive Bayes is a classification algorithm that uses the principle of Bayes theorem to make classifications.The assumption is Naive because it often does not hold. The assumption of independence implies that the model assumes that there is zero correlation among the features. Hence, the joint probability distribution  P(X,Y)\n",
    "  can be obtained from the marginal probability distributions  P(X)\n",
    "  and  P(Y)\n",
    "  simply by multiplication. We will use the above independence assumption, conditional probability rules, and Bayes theorem to develop some theory for how the Naive Bayes model works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2953c8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import  BernoulliNB\n",
    "naive_bayes =  BernoulliNB()\n",
    "nb= Pipeline([('cf', cf),('nb',  naive_bayes)]) #Create a pipeline with the bag or words features and the linearSVC\n",
    "\n",
    "nb.fit(X_train, y_train) #Fit the training data to the pipeline\n",
    "\n",
    "y_pred = nb.predict(X_test) #Make predictions with the test data\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test)) #Print the accuracy\n",
    "print('f1_score %s' % f1_score(y_test,y_pred,average='weighted')) #Print the f1-score\n",
    "print(classification_report(y_test, y_pred)) #Print the classification report\n",
    "               "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98fa8ff",
   "metadata": {},
   "source": [
    " Similarly to Logistic regression, The Naive Bayes model precision, recall and f1-score values for the pro and news class are higher.Most tweets falls under pro class and news so the model gets better at classifiying them because it has more evidence of them.\n",
    "\n",
    "- The neutral and negative class are lower.\n",
    "\n",
    "- The F1_score of Naive Bayes is lower that Logisctic regression, LVC and Decision Trees. But is accuracy is higher than Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3fa73f6",
   "metadata": {},
   "source": [
    "### F. K Nearest Neighbours - Bag of words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302e5e9c",
   "metadata": {},
   "source": [
    "In principle, this algorithm works by assigning the majority class of the N closest neighbors to the current data point. As such, absolutely no training is required for the algorithm! All we do is choose K (i.e. the number of neighbors to consider), choose a distance function to calculate proximity and we're good to go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3a5a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(10)\n",
    "cnb= Pipeline([('cf', cf),('cnb', knn)]) #Create a pipeline with the bag or words features and the linearSVC\n",
    "\n",
    "cnb.fit(X_train, y_train) #Fit the training data to the pipeline\n",
    "\n",
    "y_pred = cnb.predict(X_test) #Make predictions with the test data\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test)) #Print the accuracy\n",
    "print('f1_score %s' % f1_score(y_test,y_pred,average='weighted')) #Print the f1-score\n",
    "print(classification_report(y_test, y_pred)) #Print the classification report\n",
    "               "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa1ce45",
   "metadata": {},
   "source": [
    "The Key neighbours precision, recall and f1-score values for all the sentiments is lower than 50%  making the leaste perfoming model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491f6302",
   "metadata": {},
   "source": [
    "### A. Logistic Regression - TF – IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c433a64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(C=50,multi_class='ovr') #Call logistic regression model and assign variable 'model'\n",
    "\n",
    "clf_sam = Pipeline([('tfidf', tfidf), ('clf', model)]) #Create a pipeline with the logistic model and tf-idf vectorizer\n",
    "\n",
    "\n",
    "clf_sam.fit(X_train, y_train) #Fit the training set\n",
    "\n",
    "y_pred= clf_sam.predict(X_test) #Fit the test set\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test)) #Print accuracy\n",
    "print('f1_score %s' % f1_score(y_test,y_pred,average='weighted')) #Print f1 score\n",
    "print(classification_report(y_test, y_pred)) #Print classification report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7731bf0",
   "metadata": {},
   "source": [
    " Similarly to Logistic regression bag of words, The TF – IDF model precision, recall and f1-score values for the pro and news class are higher.Most tweets falls under pro class and news so the model gets better at classifiying them because it has more evidence of them.\n",
    "\n",
    "- The neutral and negative class are lower.\n",
    "\n",
    "- The perfomance of TF – IDF is lower that Logisctic regression bag words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c1d918",
   "metadata": {},
   "source": [
    "### 5.1.B.2. Linear SVC - TF – IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5279887",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create a pipeline and fit it with a Linear Support Vector Classifier\n",
    "\n",
    "classifier = LinearSVC() #Call LinearSVC and assign the variable 'classifier'\n",
    "\n",
    "clft = Pipeline([('tfidf', tfidf), ('clf', classifier)]) #Create a pipeline with the tdidf\n",
    "\n",
    "clft.fit(X_train, y_train) #Fit the model\n",
    "y_pred = clft.predict(X_test) #Make predictions and assign the variable 'y_pred'\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test)) #Print the accuracy\n",
    "print('f1_score %s' % f1_score(y_test,y_pred,average='weighted')) #Print the f1-score\n",
    "print(classification_report(y_test, y_pred)) #Print the classification report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.B.2. Linear SVC - TF – IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create a pipeline and fit it with a Linear Support Vector Classifier\n",
    "\n",
    "classifier = LinearSVC() #Call LinearSVC and assign the variable 'classifier'\n",
    "\n",
    "clft = Pipeline([('tfidf', tfidf), ('clf', classifier)]) #Create a pipeline with the tdidf\n",
    "\n",
    "clft.fit(X_train, y_train) #Fit the model\n",
    "y_pred = clft.predict(X_test) #Make predictions and assign the variable 'y_pred'\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test)) #Print the accuracy\n",
    "print('f1_score %s' % f1_score(y_test,y_pred,average='weighted')) #Print the f1-score\n",
    "print(classification_report(y_test, y_pred)) #Print the classification report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639e5d52",
   "metadata": {},
   "source": [
    "Similarly to LVC bag of words, The TF – IDF model precision, recall and f1-score values for the pro and news class are higher.Most tweets falls under pro class and news so the model gets better at classifiying them because it has more evidence of them.\n",
    "\n",
    "- The neutral and negative class are lower.\n",
    "\n",
    "- The perfomance of TF – IDF is higher that LVC bag words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9503d9",
   "metadata": {},
   "source": [
    "### 5.1.B.3. Decision Trees - TF – IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54d036d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create a pipeline and fit it with a Linear Support Vector Classifier\n",
    "\n",
    "\n",
    "classifier =  DecisionTreeClassifier(random_state=42) #Call LinearSVC and assign the variable 'classifier'\n",
    "\n",
    "clf = Pipeline([('tfidf', tfidf), ('clf', classifier)]) #Create a pipeline with the tdidf\n",
    "\n",
    "clf.fit(X_train, y_train) #Fit the model\n",
    "y_pred = clf.predict(X_test) #Make predictions and assign the variable 'y_pred'\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test)) #Print the accuracy\n",
    "print('f1_score %s' % f1_score(y_test,y_pred,average='weighted')) #Print the f1-score\n",
    "print(classification_report(y_test, y_pred)) #Print the classification report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4980e6",
   "metadata": {},
   "source": [
    "Similarly to Decision Trees bag of words, The TF – IDF model precision, recall and f1-score values for the pro and news class are higher.Most tweets falls under pro class and news so the model gets better at classifiying them because it has more evidence of them.\n",
    "\n",
    "- The neutral and negative class are lower.\n",
    "\n",
    "- The perfomance of TF – IDF is slighly lower Decison Trees bag words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e49df81",
   "metadata": {},
   "source": [
    "### 5.1.B.4. Naive Bayes - TF – IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f39b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create a pipeline and fit it with a Linear Support Vector Classifier\n",
    "\n",
    "classifier = BernoulliNB() #Call LinearSVC and assign the variable 'classifier'\n",
    "\n",
    "clf = Pipeline([('tfidf', tfidf), ('clf', classifier)]) #Create a pipeline with the tdidf\n",
    "\n",
    "clf.fit(X_train, y_train) #Fit the model\n",
    "y_pred = clf.predict(X_test) #Make predictions and assign the variable 'y_pred'\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test)) #Print the accuracy\n",
    "print('f1_score %s' % f1_score(y_test,y_pred,average='weighted')) #Print the f1-score\n",
    "print(classification_report(y_test, y_pred)) #Print the classification report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7fc8aa",
   "metadata": {},
   "source": [
    "The TF – IDF model precision, recall and f1-score values for the pro and news class are higher.Most tweets falls under pro class and news so the model gets better at classifiying them because it has more evidence of them.\n",
    "\n",
    "- The neutral and negative class are lower.\n",
    "\n",
    "- The perfomance of TF – IDF is the same as Naive Bayes bag words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9df84c",
   "metadata": {},
   "source": [
    "### 5.1.B.5. K Nearest Neighbour - TF – IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72752f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create a pipeline and fit it with a Linear Support Vector Classifier\n",
    "\n",
    "\n",
    "Classifier =  KNeighborsClassifier(50) #Call LinearSVC and assign the variable 'classifier'\n",
    "\n",
    "clf = Pipeline([('tfidf', tfidf), ('clf', classifier)]) #Create a pipeline with the tdidf\n",
    "\n",
    "clf.fit(X_train, y_train) #Fit the model\n",
    "y_pred = clf.predict(X_test) #Make predictions and assign the variable 'y_pred'\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test)) #Print the accuracy\n",
    "print('f1_score %s' % f1_score(y_test,y_pred,average='weighted')) #Print the f1-score\n",
    "print(classification_report(y_test, y_pred)) #Print the classification report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c9a3a8",
   "metadata": {},
   "source": [
    "The TF – IDF model precision, recall and f1-score values for the pro and news class are higher.Most tweets falls under pro class and news so the model gets better at classifiying them because it has more evidence of them.\n",
    "\n",
    "- The neutral and negative class are lower.\n",
    "\n",
    "- The perfomance of TF – IDF is higher Naive Bayes bag words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6b39cc",
   "metadata": {},
   "source": [
    "# 5.B Dealing with Imbalances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44149f1",
   "metadata": {},
   "source": [
    "Class imbalance occurs when the number of observations across different class labels are unevenly distributed. In training our classification model, it is preferable for all classes to have a relatively even split of observations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a2cf64",
   "metadata": {},
   "outputs": [],
   "source": [
    "senti_counts = df_train[\"sentiment\"].value_counts()\n",
    "news = senti_counts[2] \n",
    "pro = senti_counts[1]   \n",
    "neutral = senti_counts[0]\n",
    "anti = senti_counts[-1]  \n",
    "\n",
    "plt.figure( figsize=(9,4))\n",
    "plt.barh(['News ','Pro','Neutral','Anti'], [news,pro,neutral,anti]) # horizontal bar graph to compare classes of tweets.\n",
    "plt.colours = ['red', 'green', 'blue', 'orange']\n",
    "plt.xlabel('Count of Tweets') #X-label of the data\n",
    "plt.ylabel('Sentiment Classification') #Y_label of the data \n",
    "plt.title('Distribution of Classes In The Dataset') #Give the data a title 'Dataset lables distribution'\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ef0805",
   "metadata": {},
   "source": [
    "Resampling\n",
    "two variants of the most common method available: resampling. Put simply, resampling methods involve modifying the number of observations in each class as follows:\n",
    "\n",
    "- Downsampling - taking a random subset of the majority class small enough to match the number of observations in the minority class.\n",
    "\n",
    "- Upsampling - taking repeated random samples from the minority class until we have as many observations as the majority class. This grows the size of the minority class by effectively duplicating observations at random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ddc317",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the resampling module\n",
    "from sklearn.utils import resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4368cea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_majority = df_train.copy()[df_train['sentiment'] == 1]\n",
    "df_minority1 = df_train.copy()[df_train['sentiment'] == 0]\n",
    "df_minority2 = df_train.copy()[df_train['sentiment'] == 2]\n",
    "df_minority3 = df_train.copy()[df_train['sentiment'] == -1]\n",
    "\n",
    "\n",
    "# Downsample majority class\n",
    "df_majority_downsampled = resample(df_majority, \n",
    "                                 replace=False,    # sample without replacement\n",
    "                                 n_samples=5000,     # Using a benchmark of 3640\n",
    "                                 random_state=123) # reproducible results\n",
    "#Upsampling the least minority class\n",
    "df_minority_up = resample(df_minority1, \n",
    "                        replace=True,    # sample without replacement\n",
    "                        n_samples=5000,     # to match the second majority class\n",
    "                        random_state=123) # reproducible results\n",
    "\n",
    "df_minority_up1 = resample(df_minority2, \n",
    "                        replace=True,    # sample without replacement\n",
    "                        n_samples=5000,     # to match the second majority class\n",
    "                        random_state=123) # reproducible results\n",
    "\n",
    "df_minority_up2 = resample(df_minority3, \n",
    "                        replace=True,    # sample without replacement\n",
    "                        n_samples=5000,     # to match the second majority class\n",
    "                        random_state=123) # reproducible results\n",
    "\n",
    "# Combine minority class with downsampled majority class\n",
    "df_resampled = pd.concat([df_majority_downsampled,df_minority_up,df_minority_up1, df_minority_up2])\n",
    " \n",
    "# Display new class counts\n",
    "df_resampled.sentiment.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f941d630",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_resampled['message']\n",
    "y = df_resampled['sentiment']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a951cf6c",
   "metadata": {},
   "source": [
    "### 5.1 B Resampled Logical Regression - bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155a289a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(C=20,multi_class='ovr') #Call logistic regression model and assign variable 'model'\n",
    "\n",
    "clf_sam1 = Pipeline([('cf', cf), ('clf', model)]) #Create a pipeline with the logistic model and bag-of-words\n",
    "\n",
    "\n",
    "clf_sam1.fit(X_train, y_train) #Fit the training set\n",
    "\n",
    "y_pred= clf_sam1.predict(X_test) #Fit the test set\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test)) #Print accuracy\n",
    "print('f1_score %s' % f1_score(y_test,y_pred,average='weighted')) #Print f1 score\n",
    "print(classification_report(y_test, y_pred)) #Print classification report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d09052",
   "metadata": {},
   "source": [
    "### 5.1 B Resampled Logical Regression - TF-DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "badb79f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(C=350,multi_class='ovr') #Call logistic regression model and assign variable 'model'\n",
    "\n",
    "clf_sam1 = Pipeline([('tfidf', tfidf), ('clf', model)]) #Create a pipeline with the logistic model and bag-of-words\n",
    "\n",
    "\n",
    "clf_sam1.fit(X_train, y_train) #Fit the training set\n",
    "\n",
    "y_pred= clf_sam1.predict(X_test) #Fit the test set\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test)) #Print accuracy\n",
    "print('f1_score %s' % f1_score(y_test,y_pred,average='weighted')) #Print f1 score\n",
    "print(classification_report(y_test, y_pred)) #Print classification report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281ea1f5",
   "metadata": {},
   "source": [
    " balancing the dataset has lead to a significant improvement in the model. The upsampled dataset is performing way better than the model of the original dataset. However, due to downsampling of the data, we see that the pro class f1 score and recall are significantly lower want the rest of the classes\n",
    "\n",
    "Pro class has a significantly lower f1 score now and recall score\n",
    "\n",
    "The balance now makes it easier to make predictions on the other classes and seems to be performing much better in this regards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572ec006",
   "metadata": {},
   "source": [
    "### 5.2.B Resampled Linear SVC - TF-DF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4ce2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clf_SVC= Pipeline([('cf', cf),('clf',  LinearSVC())]) #Create a pipeline with the bag or words features and the linearSVC\n",
    "\n",
    "clf_SVC.fit(X_train, y_train) #Fit the training data to the pipeline\n",
    "\n",
    "y_pred = clf_SVC.predict(X_test) #Make predictions with the test data\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test)) #Print out the accuracy\n",
    "print('f1_score %s' % f1_score(y_test,y_pred,average='weighted')) #Print out the f1 score\n",
    "print(classification_report(y_test, y_pred)) #Print out the classification report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resampled non Linear SVC - bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_SVC= Pipeline([('cf', cf),('clf',  LinearSVC())]) #Create a pipeline with the bag or words features and the linearSVC\n",
    "\n",
    "clf_SVC.fit(X_train, y_train) #Fit the training data to the pipeline\n",
    "\n",
    "y_pred = clf_SVC.predict(X_test) #Make predictions with the test data\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test)) #Print out the accuracy\n",
    "print('f1_score %s' % f1_score(y_test,y_pred,average='weighted')) #Print out the f1 score\n",
    "print(classification_report(y_test, y_pred)) #Print out the classification report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c60be28",
   "metadata": {},
   "source": [
    "explanations:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a233200",
   "metadata": {},
   "source": [
    "### 5.2.B Resampled Linear SVC - TF-ID\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b079a4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clf_SVC= Pipeline([('tfidf', tfidf),('clf',  LinearSVC())]) #Create a pipeline with the bag or words features and the linearSVC\n",
    "\n",
    "clf_SVC.fit(X_train, y_train) #Fit the training data to the pipeline\n",
    "\n",
    "y_pred = clf_SVC.predict(X_test) #Make predictions with the test data\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test)) #Print out the accuracy\n",
    "print('f1_score %s' % f1_score(y_test,y_pred,average='weighted')) #Print out the f1 score\n",
    "print(classification_report(y_test, y_pred)) #Print out the classification report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce3f6f4",
   "metadata": {},
   "source": [
    "The model for linear SVC seems to be getting a high accuracy and high f1 score, which is good however it still struggles to locate the pro sentiments as good as it does the other models.\n",
    "\n",
    "Pro sentiments are and news sentiments are lower on the precisioin but news seem to have high recall and high f1 rating.\n",
    "\n",
    "Neutral seems to be consistent accorss all predictions\n",
    "\n",
    "Negative sentiments seem to be performing very weel, more than all the classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a368d2",
   "metadata": {},
   "source": [
    "### non Linear SVC - TF-DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713a5733",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create a pipeline and fit it with a  Support Vector Classifier\n",
    "from sklearn.svm import SVC #Import SVC from sklearn \n",
    "\n",
    "classifier = SVC(kernel='rbf', C=60, gamma='scale') #Call the SVC with the kernel='rbf' parameter\n",
    "\n",
    "clf_nlsvc = Pipeline([('tfidf', tfidf), ('clf', classifier)]) #Add the SVC model to the pipeline\n",
    "\n",
    "clf_nlsvc.fit(X_train, y_train) #Fit the training data\n",
    "y_pred = clf_nlsvc.predict(X_test) #Make predictions to the test set and assign the variable 'y_pred'\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test)) #Print the accuracy\n",
    "print('f1_score %s' % f1_score(y_test,y_pred,average='weighted')) #Print the f1 score\n",
    "print(classification_report(y_test, y_pred)) #Print out the classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462104e2",
   "metadata": {},
   "source": [
    "### 5.3 B Resampled Decision Trees -  bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0df898",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "knn = DecisionTreeClassifier(random_state=42)\n",
    "cnt= Pipeline([('cf', cf),('cnt', knn)]) #Create a pipeline with the bag or words features and the linearSVC\n",
    "\n",
    "cnb.fit(X_train, y_train) #Fit the training data to the pipeline\n",
    "\n",
    "y_pred = cnb.predict(X_test) #Make predictions with the test data\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test)) #Print the accuracy\n",
    "print('f1_score %s' % f1_score(y_test,y_pred,average='weighted')) #Print the f1-score\n",
    "print(classification_report(y_test, y_pred)) #Print the classification report\n",
    "               "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af8a790",
   "metadata": {},
   "source": [
    "### 5.3 B Resampled Decision Trees - TF_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12fe4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "knn = DecisionTreeClassifier(random_state=42)\n",
    "cntt= Pipeline([('tfidf', tfidf),('cnb', knn)]) #Create a pipeline with the bag or words features and the linearSVC\n",
    "\n",
    "cnb.fit(X_train, y_train) #Fit the training data to the pipeline\n",
    "\n",
    "y_pred = cnb.predict(X_test) #Make predictions with the test data\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test)) #Print the accuracy\n",
    "print('f1_score %s' % f1_score(y_test,y_pred,average='weighted')) #Print the f1-score\n",
    "print(classification_report(y_test, y_pred)) #Print the classification report\n",
    "               "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02061a38",
   "metadata": {},
   "source": [
    "The model for linear SVC seems to be getting a high accuracy and high f1 score, which is good however it still struggles to locate the pro sentiments as good as it does the other models.\n",
    "\n",
    "Pro sentiments are and news sentiments are lower on the precisioin but news seem to have high recall and high f1 rating.\n",
    "\n",
    "Neutral seems to be consistent accorss all predictions\n",
    "\n",
    "Negative sentiments seem to be performing very weel, more than all the classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600510f5",
   "metadata": {},
   "source": [
    "### 5.4. B Resampled Naive Bayes - Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ae3a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_bayes =  BernoulliNB()\n",
    "cnb= Pipeline([('cf', cf),('cnb',  naive_bayes)]) #Create a pipeline with the bag or words features and the linearSVC\n",
    "\n",
    "cnb.fit(X_train, y_train) #Fit the training data to the pipeline\n",
    "\n",
    "y_pred = cnb.predict(X_test) #Make predictions with the test data\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test)) #Print the accuracy\n",
    "print('f1_score %s' % f1_score(y_test,y_pred,average='weighted')) #Print the f1-score\n",
    "print(classification_report(y_test, y_pred)) #Print the classification report\n",
    "               "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70fd4f7",
   "metadata": {},
   "source": [
    "### 5.4. B Resampled Naive Bayes - TF-DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12690af",
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_bayes =  BernoulliNB()\n",
    "cnb= Pipeline([('tfidf', tfidf),('cnb',  naive_bayes)]) #Create a pipeline with the bag or words features and the linearSVC\n",
    "\n",
    "cnb.fit(X_train, y_train) #Fit the training data to the pipeline\n",
    "\n",
    "y_pred = cnb.predict(X_test) #Make predictions with the test data\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test)) #Print the accuracy\n",
    "print('f1_score %s' % f1_score(y_test,y_pred,average='weighted')) #Print the f1-score\n",
    "print(classification_report(y_test, y_pred)) #Print the classification report\n",
    "               "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d81a5e5",
   "metadata": {},
   "source": [
    "The model for linear SVC seems to be getting a high accuracy and high f1 score, which is good however it still struggles to locate the pro sentiments as good as it does the other models.\n",
    "\n",
    "Pro sentiments are and news sentiments are lower on the precisioin but news seem to have high recall and high f1 rating.\n",
    "\n",
    "Neutral seems to be consistent accorss all predictions\n",
    "\n",
    "Negative sentiments seem to be performing very weel, more than all the classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96488a75",
   "metadata": {},
   "source": [
    "### 5.5. B Resampled KNN - bag of words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6c2aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "knn = KNeighborsClassifier(10)\n",
    "cnk= Pipeline([('cf', cf),('cnk', knn)]) #Create a pipeline with the bag or words features and the linearSVC\n",
    "\n",
    "cnk.fit(X_train, y_train) #Fit the training data to the pipeline\n",
    "\n",
    "y_pred = cnk.predict(X_test) #Make predictions with the test data\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test)) #Print the accuracy\n",
    "print('f1_score %s' % f1_score(y_test,y_pred,average='weighted')) #Print the f1-score\n",
    "print(classification_report(y_test, y_pred)) #Print the classification report\n",
    "               "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59006e5f",
   "metadata": {},
   "source": [
    "### 5.5. B Resampled KNN - TF_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55aae978",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "knn = KNeighborsClassifier(10)\n",
    "cnkt= Pipeline([('tfidf', tfidf),('cnkt', knn)]) #Create a pipeline with the bag or words features and the linearSVC\n",
    "\n",
    "cnkt.fit(X_train, y_train) #Fit the training data to the pipeline\n",
    "\n",
    "y_pred = cnkt.predict(X_test) #Make predictions with the test data\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test)) #Print the accuracy\n",
    "print('f1_score %s' % f1_score(y_test,y_pred,average='weighted')) #Print the f1-score\n",
    "print(classification_report(y_test, y_pred)) #Print the classification r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1119afd3",
   "metadata": {},
   "source": [
    "The model for linear SVC seems to be getting a high accuracy and high f1 score, which is good however it still struggles to locate the pro sentiments as good as it does the other models.\n",
    "\n",
    "Pro sentiments are and news sentiments are lower on the precisioin but news seem to have high recall and high f1 rating.\n",
    "\n",
    "Neutral seems to be consistent accorss all predictions\n",
    "\n",
    "Negative sentiments seem to be performing very weel, more than all the classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c9887c",
   "metadata": {},
   "source": [
    "### Kaggle Submission Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d34edae",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = df_test['message'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de6aba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y3_predict = clf_nlsvc.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53dc1ca7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_test['sentiment'] = y3_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9d9c38",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_test['sentiment'] = df_test['sentiment'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8c0c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[['tweetid', 'sentiment']].to_csv('nonlinearSVCBalanced-unclean10.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae4913e",
   "metadata": {},
   "source": [
    "<a id=\"six\"></a>\n",
    "\n",
    "## 6. Model Performance\n",
    "\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "\n",
    "| ⚡ Description: Model performance ⚡                                                                                                                                      |\n",
    "| :------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |\n",
    "| In this section you are required to compare the relative performance of the various trained ML models on a holdout dataset and comment on what model is the best and why. |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'Model': ['Logistical Regression', 'Linear SVC', 'Decision trees', 'Naive Bayes', 'Key Nearest Neighbour'],\n",
    "    'imbalanced Acurracy Score BOW ': [74.68, 72.68, 65.89, 69.21, 39.53],\n",
    "    'imbalanced f1_score BOW': [73.99, 72.55, 65.89, 63.17, 41.91],'imbalanced Acurracy Score TF-DF ': [74.2, 74.74, 61.91, 69.21, 69.21],\n",
    "    'imbalanced f1_score TF-DF': [73.53, 73.8, 61.09, 63.17, 63.17]}\n",
    "\n",
    "summary_table = pd.DataFrame(data)\n",
    "\n",
    "summary_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ef81df",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'Model': ['Logistical Regression', 'Linear SVC','non Linear SVC', 'Decision trees', 'Naive Bayes', 'Key Nearest Neighbour'],\n",
    "    'balanced Acurracy Score BOW ': [88.98, 88.73, 0.10, 53.00, 82.92, 53.00],\n",
    "    'balanced f1_score BOW': [88.73, 88.47, 0.10, 63.00, 82.53, 50.63],'balanced Acurracy Score TF-DF ': [89.47, 88.72, 90.71, 53.00, 82.92, 67.30],\n",
    "    'balanced f1_score TF-DF': [89.27, 88.47, 90.73, 50.63, 82.53, 66.65]}\n",
    "\n",
    "summary_table = pd.DataFrame(data)\n",
    "\n",
    "summary_table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e58294",
   "metadata": {},
   "source": [
    "<a id=\"seven\"></a>\n",
    "\n",
    "## 7. Model Explanations\n",
    "\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "\n",
    "| ⚡ Description: Model explanation ⚡                                                                                                                                                                              |\n",
    "| :---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| In this section, you are required to discuss how the best performing model works in a simple way so that both technical and non-technical stakeholders can grasp the intuition behind the model's inner workings. |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af89ada",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
